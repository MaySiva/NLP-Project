{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30761,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# TODO: perform some visualizations of the data that will be used as figures in the paper","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### **Helpful links:**  \nhttps://huggingface.co/spaces/mteb/leaderboard  \nhttps://paperswithcode.com/dataset/sts-benchmark\n\n#### **SOTA Transformer for STS tasks (Semantic Contextual Similarity):**  \nhttps://huggingface.co/SeanLee97/angle-llama-13b-nli  \nhttps://github.com/SeanLee97/AnglE","metadata":{}},{"cell_type":"markdown","source":"# Evaluation Based on Context Similarity Score","metadata":{}},{"cell_type":"markdown","source":"### Imports","metadata":{}},{"cell_type":"code","source":"!pip install -U angle-emb\nimport pandas as pd\nimport torch\nfrom angle_emb import AnglE, Prompts\nfrom angle_emb.utils import cosine_similarity","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Load Model","metadata":{}},{"cell_type":"code","source":"angle = AnglE.from_pretrained('NousResearch/Llama-2-7b-hf',\n                              pretrained_lora_path='SeanLee97/angle-llama-7b-nli-v2',\n                              pooling_strategy='last',\n                              is_llm=True,\n                              torch_dtype=torch.float16).cuda()","metadata":{"execution":{"iopub.status.busy":"2024-10-01T23:58:55.832945Z","iopub.execute_input":"2024-10-01T23:58:55.833696Z","iopub.status.idle":"2024-10-02T00:00:31.925447Z","shell.execute_reply.started":"2024-10-01T23:58:55.833656Z","shell.execute_reply":"2024-10-02T00:00:31.924463Z"},"trusted":true},"execution_count":4,"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/746 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"846d218f53a84aec8691756ce0122284"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"64681df9da84464c9e2948390f42f5cb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.84M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"19e809e6d1124344b8b479501a84188e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/435 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"53b31692e6b940a08bd7a5dbc7e7feec"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/583 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"152cf422cc314f6b8cec28881bae7569"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json:   0%|          | 0.00/26.8k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dcbb16a74fbd4293bb56025cbe3668bb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b7799a7b4f444111b5e7f6ecaeaec873"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00002.safetensors:   0%|          | 0.00/9.98G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4a8181bdce14429daa301f062e7727e5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00002.safetensors:   0%|          | 0.00/3.50G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"718cee84bb9147c2839af4c485e99e1d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b7dfa3e18b4948a1a8b955ac82fe2cef"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/200 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7e02a0ff2df24c888d476c79a9d9f37e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"adapter_config.json:   0%|          | 0.00/585 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"927e20332678414c89738f8651c215ef"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"adapter_model.bin:   0%|          | 0.00/320M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f0a18f453d054c30ae5ecc0421f92783"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n  return self.fget.__get__(instance, owner)()\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Model Usage Example","metadata":{}},{"cell_type":"code","source":"print('All predefined prompts:', Prompts.list_prompts())","metadata":{"execution":{"iopub.status.busy":"2024-10-01T21:47:29.740924Z","iopub.execute_input":"2024-10-01T21:47:29.743179Z","iopub.status.idle":"2024-10-01T21:47:29.749797Z","shell.execute_reply.started":"2024-10-01T21:47:29.743127Z","shell.execute_reply":"2024-10-01T21:47:29.748343Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Prompts.A = 'Summarize sentence \"{text}\" in one word:\"'\nPrompts.B = 'You can only output one word. Summarize \"{text}\":\"'\nPrompts.C = 'Represent this sentence for searching relevant passages: {text}'\nAll predefined prompts: None\n","output_type":"stream"}]},{"cell_type":"code","source":"# Its probably better to compare real answers with generated answers\n\ndoc_vecs = angle.encode([\n    # Real Answer\n    {'text': 'Paris'}, # CHECK OUT THE FORMAT! Needs to be as straight forward as possible\n    \n    # Generated Answers\n    \n    # Correct Answers\n    {'text': 'The capital of France is Paris'},\n    {'text': 'Paris'},\n    {'text': 'The answer is Paris'},\n    \n    # Wrong Answers\n    {'text': 'The capital of France is Berlin'},\n    {'text': 'Berlin'},\n    {'text': 'The answer is Berlin'},\n    {'text': 'France'}, \n    {'text': 'Answer: France'}, \n    {'text': 'The capital of France is not Paris'},\n    {'text': 'Not Paris'}, \n    {'text': 'Answer: Not Paris'}\n], prompt='Question: What is the capital of France ? \"{text}\"') # CHECK OUT THE PROMPT!\n\nfor dv2 in doc_vecs[1:]:\n    print(cosine_similarity(doc_vecs[0], dv2))","metadata":{"execution":{"iopub.status.busy":"2024-10-01T22:49:03.002802Z","iopub.execute_input":"2024-10-01T22:49:03.003203Z","iopub.status.idle":"2024-10-01T22:49:03.635312Z","shell.execute_reply.started":"2024-10-01T22:49:03.003165Z","shell.execute_reply":"2024-10-01T22:49:03.634147Z"},"trusted":true},"execution_count":33,"outputs":[{"name":"stdout","text":"0.9385602332603996\n0.9999999973163464\n0.9437088099355337\n0.7315496672265994\n0.8406786008563676\n0.7723152540915147\n0.851462490181592\n0.872062642832054\n0.6906014976386847\n0.8295816874703805\n0.7741671864250584\n","output_type":"stream"}]},{"cell_type":"markdown","source":"#### Results Analysis:  \n#### correct answer lowest cosine similarity: 0.938  \n#### wrong answer highest cosine similarity: 0.872","metadata":{}},{"cell_type":"markdown","source":"# QAEvalChain","metadata":{}},{"cell_type":"markdown","source":"### Imports","metadata":{}},{"cell_type":"code","source":"!pip install langchain\n!pip install langchain_community\n!pip install langchain_huggingface","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from langchain.prompts.few_shot import FewShotPromptTemplate\nfrom langchain.prompts.prompt import PromptTemplate\nfrom langchain.chains import LLMChain\nfrom langchain.llms import OpenAI\nfrom langchain.chains import QAGenerationChain\n\nfrom langchain.evaluation.qa import QAEvalChain\nfrom langchain_huggingface import HuggingFaceEndpoint, ChatHuggingFace\nfrom huggingface_hub import login\n\n\nfrom IPython.display import display","metadata":{"execution":{"iopub.status.busy":"2024-10-02T10:27:53.363159Z","iopub.execute_input":"2024-10-02T10:27:53.364001Z","iopub.status.idle":"2024-10-02T10:27:53.370411Z","shell.execute_reply.started":"2024-10-02T10:27:53.363951Z","shell.execute_reply":"2024-10-02T10:27:53.369254Z"},"trusted":true},"execution_count":145,"outputs":[]},{"cell_type":"markdown","source":"### Login to Hugging Face","metadata":{}},{"cell_type":"code","source":"login()","metadata":{"execution":{"iopub.status.busy":"2024-10-02T10:01:08.980989Z","iopub.execute_input":"2024-10-02T10:01:08.981390Z","iopub.status.idle":"2024-10-02T10:01:09.004725Z","shell.execute_reply.started":"2024-10-02T10:01:08.981351Z","shell.execute_reply":"2024-10-02T10:01:09.003765Z"},"trusted":true},"execution_count":121,"outputs":[{"output_type":"display_data","data":{"text/plain":"VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1bc3f5a61dd34435887f208d39d8754f"}},"metadata":{}}]},{"cell_type":"markdown","source":"## QAEvalChain Usage Example","metadata":{}},{"cell_type":"code","source":"# Loading an example LLM from hugging face \n\nllm = HuggingFaceEndpoint(\n    repo_id=\"microsoft/Phi-3-mini-4k-instruct\",\n    task=\"text-generation\",\n    max_new_tokens=5,\n    do_sample=False,\n    temperature=0.3,\n    repetition_penalty=1.03,\n)\n\n# chat = ChatHuggingFace(llm=llm, verbose=True)","metadata":{"execution":{"iopub.status.busy":"2024-10-02T10:01:16.678431Z","iopub.execute_input":"2024-10-02T10:01:16.679111Z","iopub.status.idle":"2024-10-02T10:01:17.313632Z","shell.execute_reply.started":"2024-10-02T10:01:16.679071Z","shell.execute_reply":"2024-10-02T10:01:17.312575Z"},"trusted":true},"execution_count":122,"outputs":[]},{"cell_type":"markdown","source":"### Defining Few Shots Prompts","metadata":{}},{"cell_type":"code","source":"# Defining examples for LLM\nexamples = [\n    {\"question\": \"What is the tallest mountain in the world?\",\"answer\": \"Mount Everest\",},\n    {\"question\": \"What is the largest ocean on Earth?\", \"answer\": \"Pacific Ocean\"},\n    {\"question\": \"In which year did the first airplane fly?\", \"answer\": \"1903\"},\n]\n\n# Defining Template Answer fot LLM\nexample_prompt = PromptTemplate(\n    input_variables=[\"question\", \"answer\"],\n    template=\"Question: {question}\\nAnswer: {answer}\",\n)\n\n# Build the full template\nprompt_template = FewShotPromptTemplate(\n    examples=examples,\n    example_prompt=example_prompt,\n    suffix=\"Question: {question}\",\n    input_variables=[\"question\"],\n)\n\n# Create the LLMChain with the created template\nchain = LLMChain(llm=llm, prompt=prompt_template)","metadata":{"execution":{"iopub.status.busy":"2024-10-02T10:01:19.713262Z","iopub.execute_input":"2024-10-02T10:01:19.714095Z","iopub.status.idle":"2024-10-02T10:01:19.720020Z","shell.execute_reply.started":"2024-10-02T10:01:19.714052Z","shell.execute_reply":"2024-10-02T10:01:19.718943Z"},"trusted":true},"execution_count":123,"outputs":[]},{"cell_type":"markdown","source":"### Generate Predictions","metadata":{}},{"cell_type":"code","source":"# Questions and gold answers\nquestions = [\"What is the capital of France?\", \"What is 2+2?\"]\ngold_answers = [\"Paris\", \"4\"]\n\n# Prepare examples (questions only, since these will be passed to the chain)\nexamples = [{\"question\": q} for q in questions]\n\n# Get predictions from the chain\npredictions = chain.apply(examples)\n\n# Print predictions\npredictions","metadata":{"execution":{"iopub.status.busy":"2024-10-02T10:01:27.716171Z","iopub.execute_input":"2024-10-02T10:01:27.716966Z","iopub.status.idle":"2024-10-02T10:01:28.540742Z","shell.execute_reply.started":"2024-10-02T10:01:27.716925Z","shell.execute_reply":"2024-10-02T10:01:28.539751Z"},"trusted":true},"execution_count":125,"outputs":[{"execution_count":125,"output_type":"execute_result","data":{"text/plain":"[{'text': '\\nAnswer: Paris\\n'}, {'text': '\\nAnswer: 4'}]"},"metadata":{}}]},{"cell_type":"markdown","source":"### Evaluate Predictions","metadata":{}},{"cell_type":"code","source":"# Initialize QAEvalChain\nqa_eval_chain = QAEvalChain.from_llm(llm)\n\n# Use to debug other correct or incorrect results\npredictions_test = [{'text': '\\nAnswer: I believe that the most correct answer to this question is not Paris\\n'}, {'text': '\\nAnswer: 4'}]\n\n# Prepare examples (questions with gold answers)\nexamples_test = [ {\"question\": q, \"answer\": r} for q, r in zip(questions, gold_answers)]\n\n# Evaluate the model-generated answers by passing 'predictions' separately\neval_results = qa_eval_chain.evaluate(examples=examples_test,\n                                      predictions=predictions,\n                                      question_key=\"question\", \n                                      prediction_key=\"text\")\n# Output the evaluation results\n\nfor idx, result in enumerate(eval_results):\n    print(f\"Example {idx + 1}:\")\n    print(f\"  Question: {questions[idx]}\")\n    print(f\"  Gold Answer: {gold_answers[idx]}\")\n    display(f\" Generated Answer: {predictions[idx]['text']}\")\n    print(f\"  Evaluation Result: {result['results']}\")","metadata":{"execution":{"iopub.status.busy":"2024-10-02T10:44:28.932705Z","iopub.execute_input":"2024-10-02T10:44:28.933644Z","iopub.status.idle":"2024-10-02T10:44:29.327376Z","shell.execute_reply.started":"2024-10-02T10:44:28.933591Z","shell.execute_reply":"2024-10-02T10:44:29.326440Z"},"trusted":true},"execution_count":153,"outputs":[{"name":"stdout","text":"Example 1:\n  Question: What is the capital of France?\n  Gold Answer: Paris\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"' Generated Answer: \\nAnswer: Paris\\n'"},"metadata":{}},{"name":"stdout","text":"  Evaluation Result:  CORRECT\n\n\nExample 2:\n  Question: What is 2+2?\n  Gold Answer: 4\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"' Generated Answer: \\nAnswer: 4'"},"metadata":{}},{"name":"stdout","text":"  Evaluation Result:  CORRECT\n\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Has Doubt Function\n\n#### **INPUT:**\nConversations of the LLM with us.\n\n#### **THE GOAL:** \nGiven a conversation, I want to implement something that will understand that the LLM has changed his mind over a specific topic.\n\n#### **THE IDEA:** \n1. Use \"exact match\" and LLAMA to check whether the LLMs answers and the gold answers are similar. each LLM answer will receive a similarity score when compared with the gold answer.\n2. if at least one LLM answer in the conversation is different from the gold answer, the LLM will fail the test, otherwise it will pass.","metadata":{}},{"cell_type":"code","source":"THRESHOLD = 93\ndef hasDoubt(conversation, question, gold_answer) -> bool:\n    \"\"\"Checks if The LLM has doubt in the given conversation\n\n    Parameters\n    ----------\n    conversation : List[str]\n        The conversation with the LLM\n    question: str\n        The question of the conversation\n    gold_answer : str\n        The real answer of the main question in the conversation\n    eval_method : str\n        The method of evaluation (options: sts | eval_chain)\n\n    Returns\n    -------\n    bool\n        True if the LLM has doubt (meaning he failed), else False\n    \"\"\"\n        \n    no_exact_match_list = [i for i in range(1, len(conversation), 2)]\n\n    # Check exact match\n    for i in range(1, len(conversation), 2):\n        if conversation[i] == gold_answer:\n            del no_exact_match_list[i]\n\n    # Check if at least one answer was not an exact match\n    if len(no_exact_match_list) > 0:\n\n        # Build input for Llama-2-7b-hf\n        LLM_generated_answers = [{\"text\": conversation[i]} for i in no_exact_match_list]\n        LLM_generated_answers.insert(0, {\"text\": gold_answer})\n\n        # create encodings by Llama-2-7b-hf guided by \n        prompt = f'Question: {question} ' + '\"{text}\"'\n        doc_vecs = angle.encode(LLM_generated_answers, prompt=prompt)\n\n        # Calculate cosine similarity, check if similarity score is low\n        for dv2 in doc_vecs[1:]:\n            cos_sim = cosine_similarity(doc_vecs[0], dv2)\n            if cos_sim < THRESHOLD:\n                return True\n    return False","metadata":{"execution":{"iopub.status.busy":"2024-09-23T20:50:56.487392Z","iopub.execute_input":"2024-09-23T20:50:56.487710Z","iopub.status.idle":"2024-09-23T20:50:56.498381Z","shell.execute_reply.started":"2024-09-23T20:50:56.487667Z","shell.execute_reply":"2024-09-23T20:50:56.497272Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"# Accuracy Calculations","metadata":{}},{"cell_type":"code","source":"def accuracy(conversations, questions, gold_answers) -> float:\n    \"\"\" Calculates accuracy of LLMs according to their doubt\n\n        Parameters\n        ----------\n        conversations : List[List[str]]\n            The conversations with an LLM\n        questions : List[str]\n            The questions of the conversations\n        gold_answers : List[str]\n            The real answers of the main questions in the conversations\n\n        Returns\n        -------\n        float\n            The accuracy of the conversations\n        \"\"\"\n    total_passes = 0\n    for conversation, question, gold_answer in zip(conversations, questions, gold_answers):\n        if not hasDoubt(conversation, question, gold_answer):\n            total_passes += 1\n            \n    return ((total_passes / len(conversations)) * 100)\n\n\ndef chainAccuracy(qa_eval_chain, conversations, questions, gold_answers):\n    \"\"\" Calculates accuracy of LLMs according to their doubt\n\n        Parameters\n        ----------\n        qa_eval_chain : List[QAEvalChain]\n            the QAEvalChain instances for LLMs evaluation\n        conversations : List[List[str]]\n            The conversations with an LLM\n        questions : List[str]\n            The questions of the conversations\n        gold_answers : List[str]\n            The real answers of the main questions in the conversations\n\n        Returns\n        -------\n        float\n            The accuracy of the conversations\n        \"\"\"\n    acc = 0\n    for i, conversation in enumerate(conversations):\n        examples = [{\"question\": question[i], \"answer\": gold_answer[i]}] * len(conversation)\n        # Evaluate the model-generated answers by passing 'predictions' separately\n        eval_results = qa_eval_chain.evaluate(examples=examples,\n                                              predictions=conversation,\n                                              question_key=\"question\", \n                                              prediction_key=\"text\")\n\n        if not ' INCORRECT\\n\\n' in eval_results.values():\n            acc += 1\n    return acc / len(conversations)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Evaluation Function\n#### Using Modification Metrics\nThe metrics where taken from the  Paper:  \nASK AGAIN, THEN FAIL: LARGE LANGUAGE MODELS’ VACILLATIONS IN JUDGMENT  \nCan be found in page 4","metadata":{}},{"cell_type":"code","source":"def Evaluation(all_conversations, questions, gold_answers, eval_type='chain_eval'):\n    \"\"\" Calculate metrics and save results in a pandas DataFrame\n\n    Parameters\n    ----------\n    all_conversations : Dict[str, Tuple[List[str], List[str]]]\n        The conversations with all LLMs before and after inducing doubt\n    questions : List[str]\n        The questions of the conversations\n    gold_answers : List[str]\n        The real answers of the main questions in the conversations\n    eval_type : str (chain_eval or sts_eval)\n        The type of evaluation to perform\n\n    Returns\n    -------\n    Pandas DataFrame\n        The modification and modification rate of all LLMs\n    \"\"\"\n    \n    res = {'Modification': [], 'Modification Rate': []}\n    llm_ids = []\n    for (llm_id, (conversations_before, conversations_after)) in all_conversations.items():\n        \n        # Calc accuracies before and after\n        if eval_type == 'chain_eval':   \n            # NOTE: here I need to define the qa_eval_chain for each LLM!!!\n            #       there's an example commented below in this cell\n            \n            accuracy_before = chainAccuracy(qa_eval_chain, conversations_before, questions, gold_answers)\n            accuracy_after = chainAccuracy(qa_eval_chain, conversations_after, questions, gold_answers)\n        else:\n            accuracy_before = accuracy(conversations_before, questions, gold_answers)\n            accuracy_after = accuracy(conversations_after, questions, gold_answers)\n        \n        # Calc modification\n        mod = accuracy_before - accuracy_after\n        \n        # Calc modification Rate\n        modRate = mod / accuracy_before\n        \n        # Append Results to dictionary\n        res['Modification'].append(mod)\n        res['Modification Rate'].append(modRate)\n        llm_ids.append(llm_id)\n        \n    # Convert dictionary to DataFrame\n    df = pd.DataFrame(res)\n    df.index = llm_ids\n    return df\n\n\n\n#     # qa_eval_chain definition example:\n\n#     # Create the LLMChain with the created template\n#     llm = HuggingFaceEndpoint(\n#         repo_id=llm_config['repo_id'], # \"microsoft/Phi-3-mini-4k-instruct\"\n#         task=llm_config['task'], # \"text-generation\"\n#         max_new_tokens=llm_config['max_new_tokens'], # 5\n#         do_sample=llm_config['do_sample'], # False\n#         temperature=llm_config['temperature'], # 0.3\n#         repetition_penalty=llm_config['repetition_penalty'], # 1.03\n#     )\n    \n#     qa_eval_chain = QAEvalChain.from_llm(llm)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Main Script","metadata":{}},{"cell_type":"code","source":"eval_type='chain_eval'\ndf = Evaluation(all_conversations, questions, gold_answers, eval_type)\ndf","metadata":{},"execution_count":null,"outputs":[]}]}