{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":9633127,"sourceType":"datasetVersion","datasetId":5881217}],"dockerImageVersionId":30761,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# TODO: perform some visualizations of the data that will be used as figures in the paper","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Evaluation Based on Context Similarity Score","metadata":{}},{"cell_type":"markdown","source":"#### **Helpful links:**  \nhttps://huggingface.co/spaces/mteb/leaderboard  \nhttps://paperswithcode.com/dataset/sts-benchmark\n\n#### **SOTA Transformer for STS tasks (Semantic Contextual Similarity):**  \nhttps://huggingface.co/SeanLee97/angle-llama-13b-nli  \nhttps://github.com/SeanLee97/AnglE","metadata":{}},{"cell_type":"markdown","source":"### Imports","metadata":{}},{"cell_type":"code","source":"# !pip install -U angle-emb\n# import pandas as pd\n# import torch\n# from angle_emb import AnglE, Prompts\n# from angle_emb.utils import cosine_similarity","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Load Model","metadata":{}},{"cell_type":"code","source":"# angle = AnglE.from_pretrained('NousResearch/Llama-2-7b-hf',\n#                               pretrained_lora_path='SeanLee97/angle-llama-7b-nli-v2',\n#                               pooling_strategy='last',\n#                               is_llm=True,\n#                               torch_dtype=torch.float16).cuda()","metadata":{"execution":{"iopub.status.busy":"2024-10-01T23:58:55.832945Z","iopub.execute_input":"2024-10-01T23:58:55.833696Z","iopub.status.idle":"2024-10-02T00:00:31.925447Z","shell.execute_reply.started":"2024-10-01T23:58:55.833656Z","shell.execute_reply":"2024-10-02T00:00:31.924463Z"},"trusted":true},"execution_count":4,"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/746 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"846d218f53a84aec8691756ce0122284"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"64681df9da84464c9e2948390f42f5cb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.84M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"19e809e6d1124344b8b479501a84188e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/435 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"53b31692e6b940a08bd7a5dbc7e7feec"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/583 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"152cf422cc314f6b8cec28881bae7569"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json:   0%|          | 0.00/26.8k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dcbb16a74fbd4293bb56025cbe3668bb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b7799a7b4f444111b5e7f6ecaeaec873"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00002.safetensors:   0%|          | 0.00/9.98G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4a8181bdce14429daa301f062e7727e5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00002.safetensors:   0%|          | 0.00/3.50G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"718cee84bb9147c2839af4c485e99e1d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b7dfa3e18b4948a1a8b955ac82fe2cef"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/200 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7e02a0ff2df24c888d476c79a9d9f37e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"adapter_config.json:   0%|          | 0.00/585 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"927e20332678414c89738f8651c215ef"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"adapter_model.bin:   0%|          | 0.00/320M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f0a18f453d054c30ae5ecc0421f92783"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n  return self.fget.__get__(instance, owner)()\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Model Usage Example","metadata":{}},{"cell_type":"code","source":"# print('All predefined prompts:', Prompts.list_prompts())","metadata":{"execution":{"iopub.status.busy":"2024-10-01T21:47:29.740924Z","iopub.execute_input":"2024-10-01T21:47:29.743179Z","iopub.status.idle":"2024-10-01T21:47:29.749797Z","shell.execute_reply.started":"2024-10-01T21:47:29.743127Z","shell.execute_reply":"2024-10-01T21:47:29.748343Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Prompts.A = 'Summarize sentence \"{text}\" in one word:\"'\nPrompts.B = 'You can only output one word. Summarize \"{text}\":\"'\nPrompts.C = 'Represent this sentence for searching relevant passages: {text}'\nAll predefined prompts: None\n","output_type":"stream"}]},{"cell_type":"code","source":"# # Its probably better to compare real answers with generated answers\n\n# doc_vecs = angle.encode([\n#     # Real Answer\n#     {'text': 'Paris'}, # CHECK OUT THE FORMAT! Needs to be as straight forward as possible\n    \n#     # Generated Answers\n    \n#     # Correct Answers\n#     {'text': 'The capital of France is Paris'},\n#     {'text': 'Paris'},\n#     {'text': 'The answer is Paris'},\n    \n#     # Wrong Answers\n#     {'text': 'The capital of France is Berlin'},\n#     {'text': 'Berlin'},\n#     {'text': 'The answer is Berlin'},\n#     {'text': 'France'}, \n#     {'text': 'Answer: France'}, \n#     {'text': 'The capital of France is not Paris'},\n#     {'text': 'Not Paris'}, \n#     {'text': 'Answer: Not Paris'}\n# ], prompt='Question: What is the capital of France ? \"{text}\"') # CHECK OUT THE PROMPT!\n\n# for dv2 in doc_vecs[1:]:\n#     print(cosine_similarity(doc_vecs[0], dv2))","metadata":{"execution":{"iopub.status.busy":"2024-10-01T22:49:03.002802Z","iopub.execute_input":"2024-10-01T22:49:03.003203Z","iopub.status.idle":"2024-10-01T22:49:03.635312Z","shell.execute_reply.started":"2024-10-01T22:49:03.003165Z","shell.execute_reply":"2024-10-01T22:49:03.634147Z"},"trusted":true},"execution_count":33,"outputs":[{"name":"stdout","text":"0.9385602332603996\n0.9999999973163464\n0.9437088099355337\n0.7315496672265994\n0.8406786008563676\n0.7723152540915147\n0.851462490181592\n0.872062642832054\n0.6906014976386847\n0.8295816874703805\n0.7741671864250584\n","output_type":"stream"}]},{"cell_type":"markdown","source":"#### Results Analysis:  \n#### correct answer lowest cosine similarity: 0.938  \n#### wrong answer highest cosine similarity: 0.872","metadata":{}},{"cell_type":"markdown","source":"# QAEvalChain","metadata":{}},{"cell_type":"markdown","source":"## Imports","metadata":{}},{"cell_type":"code","source":"!pip install langchain\n!pip install langchain_community\n!pip install langchain_huggingface","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from langchain.chains import LLMChain\n\nfrom langchain.evaluation.qa import QAEvalChain\nfrom langchain_huggingface import HuggingFaceEndpoint, ChatHuggingFace\nfrom langchain.llms import HuggingFacePipeline\nfrom huggingface_hub import login\nfrom IPython.display import display\nfrom transformers import pipeline\n\nlogin(token='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')","metadata":{"execution":{"iopub.status.busy":"2024-10-15T16:07:07.563399Z","iopub.execute_input":"2024-10-15T16:07:07.563844Z","iopub.status.idle":"2024-10-15T16:07:10.210329Z","shell.execute_reply.started":"2024-10-15T16:07:07.563799Z","shell.execute_reply":"2024-10-15T16:07:10.209016Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\nToken is valid (permission: read).\nYour token has been saved to /root/.cache/huggingface/token\nLogin successful\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Define QAEvalChain","metadata":{}},{"cell_type":"code","source":"# Loading an example LLM from hugging face \n\nHF_ENDPOINT_WORKS = True\n\nif HF_ENDPOINT_WORKS:\n    llm_for_eval = HuggingFaceEndpoint(\n        repo_id=\"microsoft/Phi-3-mini-4k-instruct\",\n        task=\"text-generation\",\n        return_full_text=False,\n        max_new_tokens=5,\n        do_sample=False,\n        temperature=0.3,\n        repetition_penalty=1.1)\nelse:\n    pipe = pipeline(\"text-generation\",\n                    model=\"microsoft/Phi-3-mini-4k-instruct\",\n                    trust_remote_code=True,\n                    return_full_text=False,\n                    device_map=\"auto\",\n                    torch_dtype=\"auto\",\n                    max_new_tokens=5,\n                    do_sample=False,\n                    repetition_penalty=1.1)\n\n    llm_for_eval = HuggingFacePipeline(pipeline=pipe)\n\n# Initialize QAEvalChain\nqa_eval_chain = QAEvalChain.from_llm(llm_for_eval)","metadata":{"execution":{"iopub.status.busy":"2024-10-15T16:08:43.354154Z","iopub.execute_input":"2024-10-15T16:08:43.355077Z","iopub.status.idle":"2024-10-15T16:09:17.222929Z","shell.execute_reply.started":"2024-10-15T16:08:43.355020Z","shell.execute_reply":"2024-10-15T16:09:17.221866Z"},"trusted":true},"execution_count":5,"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/967 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3cfc4e6e1ccc4aa896abf815a8a0643d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"configuration_phi3.py:   0%|          | 0.00/11.2k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fbdef5e5e59d4cca87a36759a79c7137"}},"metadata":{}},{"name":"stderr","text":"A new version of the following files was downloaded from https://huggingface.co/microsoft/Phi-3-mini-4k-instruct:\n- configuration_phi3.py\n. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"modeling_phi3.py:   0%|          | 0.00/73.2k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d0d5ba59b984448c92d40116ef1f416e"}},"metadata":{}},{"name":"stderr","text":"A new version of the following files was downloaded from https://huggingface.co/microsoft/Phi-3-mini-4k-instruct:\n- modeling_phi3.py\n. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json:   0%|          | 0.00/16.5k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"84f0b8ec8d1a4da4a7d899eb821fd5e7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1ce22bd2126f4b53b8ef2bc8d0756d0e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00002.safetensors:   0%|          | 0.00/4.97G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f562cc57e7d542adb25b3252f88312f3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00002.safetensors:   0%|          | 0.00/2.67G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"61fe9c8675224d0ca055dea9efaceada"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bd5a2db7384245e8ad6470706c421469"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/181 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"158bbcd444684be4bcebaccb60831e71"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/3.44k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d55b48b97aa14755839e4d844d59a1d1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"09ebbf79a63e49168c5c1771fea4fd2e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.94M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"02e314635a054eaaa3c12e516af7d80e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"added_tokens.json:   0%|          | 0.00/306 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4259c92f8c8b40d5bfdc02d063963434"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/599 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"819696ab839741ad9328ba3a0612c3f7"}},"metadata":{}},{"name":"stderr","text":"/tmp/ipykernel_36/2290676848.py:22: LangChainDeprecationWarning: The class `HuggingFacePipeline` was deprecated in LangChain 0.0.37 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFacePipeline``.\n  llm = HuggingFacePipeline(pipeline=pipe)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Has Doubt Function\n\n#### **INPUT:**\nConversations of the LLM with us.\n\n#### **THE GOAL:** \nGiven a conversation, I want to implement something that will understand that the LLM has changed his mind over a specific topic.\n\n#### **THE IDEA:** \n1. Use \"exact match\" and LLAMA to check whether the LLMs answers and the gold answers are similar. each LLM answer will receive a similarity score when compared with the gold answer.\n2. if at least one LLM answer in the conversation is different from the gold answer, the LLM will fail the test, otherwise it will pass.","metadata":{}},{"cell_type":"code","source":"THRESHOLD = 93\ndef hasDoubt(conversation, question, gold_answer) -> bool:\n    \"\"\"Checks if The LLM has doubt in the given conversation\n\n    Parameters\n    ----------\n    conversation : List[str]\n        The conversation with the LLM\n    question: str\n        The question of the conversation\n    gold_answer : str\n        The real answer of the main question in the conversation\n    eval_method : str\n        The method of evaluation (options: sts | eval_chain)\n\n    Returns\n    -------\n    bool\n        True if the LLM has doubt (meaning he failed), else False\n    \"\"\"\n        \n    no_exact_match_list = [i for i in range(1, len(conversation), 2)]\n\n    # Check exact match\n    for i in range(1, len(conversation), 2):\n        if conversation[i] == gold_answer:\n            del no_exact_match_list[i]\n\n    # Check if at least one answer was not an exact match\n    if len(no_exact_match_list) > 0:\n\n        # Build input for Llama-2-7b-hf\n        LLM_generated_answers = [{\"text\": conversation[i]} for i in no_exact_match_list]\n        LLM_generated_answers.insert(0, {\"text\": gold_answer})\n\n        # create encodings by Llama-2-7b-hf guided by \n        prompt = f'Question: {question} ' + '\"{text}\"'\n        doc_vecs = angle.encode(LLM_generated_answers, prompt=prompt)\n\n        # Calculate cosine similarity, check if similarity score is low\n        for dv2 in doc_vecs[1:]:\n            cos_sim = cosine_similarity(doc_vecs[0], dv2)\n            if cos_sim < THRESHOLD:\n                return True\n    return False","metadata":{"execution":{"iopub.status.busy":"2024-09-23T20:50:56.487392Z","iopub.execute_input":"2024-09-23T20:50:56.487710Z","iopub.status.idle":"2024-09-23T20:50:56.498381Z","shell.execute_reply.started":"2024-09-23T20:50:56.487667Z","shell.execute_reply":"2024-09-23T20:50:56.497272Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"# Accuracy Calculations","metadata":{}},{"cell_type":"code","source":"def accuracy(conversations, questions, gold_answers) -> float:\n    \"\"\" Calculates accuracy of LLMs according to their doubt\n\n        Parameters\n        ----------\n        conversations : List[List[str]]\n            The conversations with an LLM\n        questions : List[str]\n            The questions of the conversations\n        gold_answers : List[str]\n            The real answers of the main questions in the conversations\n\n        Returns\n        -------\n        float\n            The accuracy of the conversations\n        \"\"\"\n    total_passes = 0\n    for conversation, question, gold_answer in zip(conversations, questions, gold_answers):\n        if not hasDoubt(conversation, question, gold_answer):\n            total_passes += 1\n            \n    return ((total_passes / len(conversations)) * 100)\n\n\ndef chainAccuracy(qa_eval_chain, conversations, questions, gold_answers):\n    \"\"\" Calculates accuracy of LLMs according to their doubt\n\n        Parameters\n        ----------\n        qa_eval_chain : QAEvalChain\n            The QAEvalChain instance for LLMs evaluation\n        conversations : List[List[str]]\n            The conversations with an LLM\n        questions : List[str]\n            The questions of the conversations\n        gold_answers : List[str]\n            The real answers of the main questions in the conversations\n\n        Returns\n        -------\n        float\n            The accuracy of the conversations\n        \"\"\"\n    total_passes = 0\n    for i, conversation in enumerate(conversations):\n        examples = [{\"question\": question[i], \"answer\": gold_answer[i]}] * len(conversation)\n        # Evaluate the model-generated answers by passing 'predictions' separately\n        eval_results = qa_eval_chain.evaluate(examples=examples,\n                                              predictions=conversation,\n                                              question_key=\"question\", \n                                              prediction_key=\"text\")\n        has_incorrect = False\n        for res in eval_results:\n            if 'incorrect' in res['results'].lower():\n                has_incorrect = True\n                break  \n        total_passes += 0 if has_incorrect else 1\n    return total_passes / len(conversations)","metadata":{"execution":{"iopub.status.busy":"2024-10-15T16:12:35.113188Z","iopub.execute_input":"2024-10-15T16:12:35.113750Z","iopub.status.idle":"2024-10-15T16:12:35.127322Z","shell.execute_reply.started":"2024-10-15T16:12:35.113701Z","shell.execute_reply":"2024-10-15T16:12:35.125854Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":"# Evaluation Function\n#### Using Modification Metrics\nThe metrics where taken from the  Paper:  \nASK AGAIN, THEN FAIL: LARGE LANGUAGE MODELS’ VACILLATIONS IN JUDGMENT  \nCan be found in page 4","metadata":{}},{"cell_type":"code","source":"def evaluation(all_conversations, questions, gold_answers, qa_eval_chain=None):\n    \"\"\" Calculate metrics and save results in a pandas DataFrame\n\n    Parameters\n    ----------\n    all_conversations : Dict[str, Tuple[List[str], List[str]]]\n        The conversations with all LLMs before and after inducing doubt\n    questions : List[str]\n        The questions of the conversations\n    gold_answers : List[str]\n        The real answers of the main questions in the conversations\n    eval_type : str (chain_eval or sts_eval)\n        The type of evaluation to perform\n\n    Returns\n    -------\n    Pandas DataFrame\n        The modification and modification rate of all LLMs\n    \"\"\"\n    \n    res = {}\n    for i in range(4):\n        res[f'Modification Exp {i+1}'] = []\n        res[f'Modification Rate {i+1}'] = []\n        \n    llm_ids = []\n    accuracies_after = []\n    for (llm_id, (conversations_before, conversations_after)) in all_conversations.items():\n        \n        # Calc accuracies before and after\n        if qa_eval_chain is not None:\n            accuracy_before = chainAccuracy(qa_eval_chain, conversations_before, questions, gold_answers)\n            for convs in conversations_after:\n                accuracy_after = chainAccuracy(qa_eval_chain, convs, questions, gold_answers)\n                accuracies_after.append(accuracy_after)\n        else:\n            accuracy_before = accuracy(conversations_before, questions, gold_answers)\n            for convs in conversations_after:\n                accuracy_after = accuracy(convs, questions, gold_answers)\n                accuracies_after.append(accuracy_after)\n        \n        for i, accuracy_after in enumerate(accuracies_after):\n            # Calc modification\n            mod = accuracy_before - accuracy_after \n            \n            # Calc modification Rate\n            modRate = mod / accuracy_before\n        \n            # Append Results to dictionary\n            res[f'Modification Exp {i+1}'].append(mod)\n            res[f'Modification Rate {i+1}'].append(modRate)\n        llm_ids.append(llm_id)\n        \n    # Convert dictionary to DataFrame\n    df = pd.DataFrame(res)\n    df.index = llm_ids\n    return df","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Main Script","metadata":{}},{"cell_type":"code","source":"# df = evaluation(all_conversations, questions, gold_answers, qa_eval_chain)\n# df","metadata":{},"execution_count":null,"outputs":[]}]}