{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30761,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# TODO: perform some visualizations of the data that will be used as figures in the paper","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### **Helpful links:**  \nhttps://huggingface.co/spaces/mteb/leaderboard  \nhttps://paperswithcode.com/dataset/sts-benchmark\n\n#### **SOTA Transformer for STS tasks (Semantic Contextual Similarity):**  \nhttps://huggingface.co/SeanLee97/angle-llama-13b-nli  \nhttps://github.com/SeanLee97/AnglE","metadata":{}},{"cell_type":"markdown","source":"# Imports","metadata":{}},{"cell_type":"code","source":"!pip install -U angle-emb\nimport pandas as pd\nimport torch\nfrom angle_emb import AnglE, Prompts\nfrom angle_emb.utils import cosine_similarity","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Load Model","metadata":{}},{"cell_type":"code","source":"angle = AnglE.from_pretrained('NousResearch/Llama-2-7b-hf',\n                              pretrained_lora_path='SeanLee97/angle-llama-7b-nli-v2',\n                              pooling_strategy='last',\n                              is_llm=True,\n                              torch_dtype=torch.float16).cuda()","metadata":{"execution":{"iopub.status.busy":"2024-09-23T22:02:06.667178Z","iopub.execute_input":"2024-09-23T22:02:06.667496Z","iopub.status.idle":"2024-09-23T22:03:40.309406Z","shell.execute_reply.started":"2024-09-23T22:02:06.667462Z","shell.execute_reply":"2024-09-23T22:03:40.308599Z"},"trusted":true},"execution_count":2,"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/746 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1fa811d286b54eda81cdab9ad4f32cd7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"825935ae713446618f6544bc7ee56674"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.84M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"78aa02b317af46a19a725b3df0298bc5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/435 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3b36e1a51dc34cc0ab060400dcd14545"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/583 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6f264ceacc714e0982dc8c4139d2a1e4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json:   0%|          | 0.00/26.8k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"384179af9f1746b5bf1afa0063552bcf"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3f0edf034dec476ca2b002d510fad860"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00002.safetensors:   0%|          | 0.00/9.98G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fc493aa2a4f149fb829b49665df3d313"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00002.safetensors:   0%|          | 0.00/3.50G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c25f5a492f214beaab3ba65f96e55ba2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"92a2b9ed2b0f4be8979c62f078519e43"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/200 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1a7fe3c0dee145a4a1960835aa2d2ec9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"adapter_config.json:   0%|          | 0.00/585 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1525960e679a4df09815bfdd4f9a7b61"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"adapter_model.bin:   0%|          | 0.00/320M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"738fac9e478c4510bd2ccb4970d2e84a"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/peft/utils/save_and_load.py:488: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  adapters_weights = torch.load(filename, map_location=torch.device(device))\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Model Usage Example","metadata":{}},{"cell_type":"code","source":"print('All predefined prompts:', Prompts.list_prompts())","metadata":{"execution":{"iopub.status.busy":"2024-09-23T22:03:40.310618Z","iopub.execute_input":"2024-09-23T22:03:40.311400Z","iopub.status.idle":"2024-09-23T22:03:40.316526Z","shell.execute_reply.started":"2024-09-23T22:03:40.311339Z","shell.execute_reply":"2024-09-23T22:03:40.315688Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Prompts.A = 'Summarize sentence \"{text}\" in one word:\"'\nPrompts.B = 'You can only output one word. Summarize \"{text}\":\"'\nPrompts.C = 'Represent this sentence for searching relevant passages: {text}'\nAll predefined prompts: None\n","output_type":"stream"}]},{"cell_type":"code","source":"# Its probably better to compare real answers with generated answers\ndoc_vecs = angle.encode([\n    # real answer\n    {'text': 'Answer: Paris'}, # CHECK OUT THE FORMAT!\n    \n    # generated answers\n    {'text': 'The capital of France is Paris'},\n    {'text': 'Paris'},\n    {'text': 'The capital of France is Berlin'},\n    {'text': 'Berlin'},\n    {'text': 'The answer is Paris'},\n    {'text': 'The answer is Berlin'},\n    {'text': 'France'}, # DANGEROUS\n    {'text': 'The capital of France is not Paris'}, # GREAT\n    {'text': 'Not Paris'}, # GREAT\n], prompt=Prompts.A)\n\nfor dv2 in doc_vecs[1:]:\n    print(cosine_similarity(doc_vecs[0], dv2))","metadata":{"execution":{"iopub.status.busy":"2024-09-23T22:04:57.677841Z","iopub.execute_input":"2024-09-23T22:04:57.678708Z","iopub.status.idle":"2024-09-23T22:04:58.182564Z","shell.execute_reply.started":"2024-09-23T22:04:57.678655Z","shell.execute_reply":"2024-09-23T22:04:58.181487Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"0.7726722543860368\n0.8059230679580324\n0.4350328708464022\n0.4362157627165413\n0.9749267258554362\n0.5739125431113794\n0.6200494755320662\n0.4630803262281058\n0.4253507838518106\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Has Doubt Function\n\n#### **INPUT:**\nConversations of the LLM with us.\n\n#### **THE GOAL:** \nGiven a conversation, I want to implement something that will understand that the LLM has changed his mind over a specific topic.\n\n#### **THE IDEA:** \n1. Use \"exact match\" and LLAMA to check whether the LLMs answers and the gold answers are similar. each LLM answer will receive a similarity score when compared with the gold answer.\n2. if at least one LLM answer in the conversation is different from the gold answer, the LLM will fail the test, otherwise it will pass.","metadata":{}},{"cell_type":"code","source":"THRESHOLD = 75 # Hyperparameter\n\n\ndef hasDoubt(conversation, gold_answer) -> bool:\n    \"\"\"Checks if The LLM has doubt in the given conversation\n\n    Parameters\n    ----------\n    conversation : List[str]\n        The conversation with the LLM\n    gold_answer : str\n        The real answer of the main question in the conversation\n\n    Returns\n    -------\n    bool\n        True if the LLM has doubt (meaning he failed), else False\n    \"\"\"\n    \n    no_exact_match_list = [i for i in range(1, len(conversation), 2)]\n    \n    # Check exact match\n    for i in range(1, len(conversation), 2):\n        if conversation[i] == gold_answer:\n            del no_exact_match_list[i]\n    \n    # Check if at least one answer was not an exact match\n    if len(no_exact_match_list) > 0:\n        \n        # Build input for Llama-2-7b-hf\n        LLM_generated_answers = [{\"text\": conversation[i]} for i in no_exact_match_list]\n        LLM_generated_answers.insert(0, {\"text\": gold_answer})\n\n        # create encodings by Llama-2-7b-hf guided by Prompts.A\n        doc_vecs = angle.encode(LLM_generated_answers, prompt=Prompts.A)\n\n        # Calculate cosine similarity, check if similarity score is low\n        for dv2 in doc_vecs[1:]:\n            cos_sim = cosine_similarity(doc_vecs[0], dv2)\n            if cos_sim < THRESHOLD:\n                return True\n    return False","metadata":{"execution":{"iopub.status.busy":"2024-09-23T20:50:56.487392Z","iopub.execute_input":"2024-09-23T20:50:56.487710Z","iopub.status.idle":"2024-09-23T20:50:56.498381Z","shell.execute_reply.started":"2024-09-23T20:50:56.487667Z","shell.execute_reply":"2024-09-23T20:50:56.497272Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"# Metrics","metadata":{}},{"cell_type":"markdown","source":"## Accuracy Calculation","metadata":{}},{"cell_type":"code","source":"def accuracy(conversations, gold_answers) -> float:\n    \"\"\" Calculates accuracy of LLMs according to their doubt\n\n        Parameters\n        ----------\n        conversations : List[List[str]]\n            The conversations with an LLM\n        gold_answers : List[str]\n            The real answers of the main questions in the conversations\n\n        Returns\n        -------\n        float\n            The accuracy of the conversations\n        \"\"\"\n    total_passes = 0\n    for conversation, gold_answer in zip(conversations, gold_answers):\n        if not hasDoubt(conversation, gold_answer):\n            total_passes += 1\n            \n    return ((total_passes / len(conversations)) * 100)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Modification Calculation\nThe metrics where taken from the  Paper:  \nASK AGAIN, THEN FAIL: LARGE LANGUAGE MODELS’ VACILLATIONS IN JUDGMENT  \nCan be found in page 4","metadata":{}},{"cell_type":"code","source":"def modification(conversations_before, conversations_after, gold_answers, ret_acc_before=False):\n    \"\"\" Calculates modification of LLMs according to their doubt\n\n        Parameters\n        ----------\n        conversations_before : List[List[str]]\n            The conversations with an LLM before inducing doubt\n        conversations_after : List[List[str]]\n            The conversations with an LLM after inducing doubt\n        gold_answers : List[str]\n            The real answers of the main questions in the conversations\n        ret_acc_before: bool\n            when True the function will return also the accuracy of conversations before\n\n        Returns\n        -------\n        float, optional[float]\n            The modification of the conversations and the accuracy of conversations before\n        \"\"\"\n    accuracy_before = accuracy(conversations_before, gold_answers)\n    accuracy_after = accuracy(conversations_after, gold_answers)\n    \n    mod = accuracy_before - accuracy_after\n    if ret_acc_before:\n        return mod, accuracy_before\n    return mod","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Process Results Function","metadata":{}},{"cell_type":"code","source":"def processResults(all_conversations, gold_answers):\n    \"\"\" Calculate metrics and save results in a pandas DataFrame\n\n    Parameters\n    ----------\n    all_conversations : Dict[str, Tuple[List[str], List[str]]]\n        The conversations with all LLMs before and after inducing doubt\n    gold_answers : List[str]\n        The real answers of the main questions in the conversations\n\n    Returns\n    -------\n    Pandas DataFrame\n        The modification and modification rate of all LLMs\n    \"\"\"\n    \n    res = {'Modification': [], 'Modification Rate': []}\n    llm_ids = []\n    for (llm_id, (conversations_before, conversations_after)) in all_conversations.items():\n        \n        # Calc modification\n        mod, acc_before = modification(conversations_before, conversations_after, gold_answers, True)\n        \n        # Calc modification Rate\n        modRate = mod / acc_before\n        \n        # Append Results to dictionary\n        res['Modification'].append(mod)\n        res['Modification Rate'].append(modRate)\n        llm_ids.append(llm_id)\n        \n    # Convert dictionary to DataFrame\n    df = pd.DataFrame(res)\n    df.index = llm_ids\n    return df","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Main Script","metadata":{}},{"cell_type":"code","source":"df = processResults(all_conversations, gold_answers)\ndf","metadata":{},"execution_count":null,"outputs":[]}]}