{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":9726491,"sourceType":"datasetVersion","datasetId":5889639},{"sourceId":9726641,"sourceType":"datasetVersion","datasetId":5881217}],"dockerImageVersionId":30761,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# TODO: perform some visualizations of the data that will be used as figures in the paper","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# QAEvalChain","metadata":{}},{"cell_type":"markdown","source":"## Imports","metadata":{}},{"cell_type":"code","source":"!pip install langchain\n!pip install langchain_community\n!pip install langchain_huggingface","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from langchain.evaluation.qa import QAEvalChain\nfrom langchain_huggingface import HuggingFaceEndpoint\nfrom langchain.llms import HuggingFacePipeline\nfrom huggingface_hub import login\nfrom transformers import pipeline\nimport pandas as pd\nimport pickle\nimport os\n\nlogin(token='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')","metadata":{"execution":{"iopub.status.busy":"2024-10-26T08:41:05.483561Z","iopub.execute_input":"2024-10-26T08:41:05.483896Z","iopub.status.idle":"2024-10-26T08:41:25.484570Z","shell.execute_reply.started":"2024-10-26T08:41:05.483861Z","shell.execute_reply":"2024-10-26T08:41:25.483624Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\nToken is valid (permission: read).\nYour token has been saved to /root/.cache/huggingface/token\nLogin successful\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Define QAEvalChain","metadata":{}},{"cell_type":"code","source":"# Loading an example LLM from hugging face \n\nHF_ENDPOINT_WORKS = False\n\nif HF_ENDPOINT_WORKS:\n    llm_for_eval = HuggingFaceEndpoint(\n        repo_id=\"microsoft/Phi-3.5-mini-instruct\",\n        task=\"text-generation\",\n        return_full_text=False,\n        max_new_tokens=5,\n        do_sample=False,\n        temperature=0.3,\n        repetition_penalty=1.1)\nelse:\n    pipe = pipeline(\"text-generation\",\n                    model=\"microsoft/Phi-3.5-mini-instruct\",\n                    trust_remote_code=True,\n                    return_full_text=False,\n                    device_map=\"auto\",\n                    torch_dtype=\"auto\",\n                    max_new_tokens=5,\n                    do_sample=False,\n                    repetition_penalty=1.1)\n\n    llm_for_eval = HuggingFacePipeline(pipeline=pipe)\n\n# Initialize QAEvalChain\nqa_eval_chain = QAEvalChain.from_llm(llm_for_eval)","metadata":{"execution":{"iopub.status.busy":"2024-10-26T08:41:25.485871Z","iopub.execute_input":"2024-10-26T08:41:25.486547Z","iopub.status.idle":"2024-10-26T08:44:34.101705Z","shell.execute_reply.started":"2024-10-26T08:41:25.486511Z","shell.execute_reply":"2024-10-26T08:44:34.100739Z"},"trusted":true},"execution_count":3,"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/3.45k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ee42c20569344ff5b36005145ba1c51b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"configuration_phi3.py:   0%|          | 0.00/11.2k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9430535fdf334cafb211b065798f942a"}},"metadata":{}},{"name":"stderr","text":"A new version of the following files was downloaded from https://huggingface.co/microsoft/Phi-3.5-mini-instruct:\n- configuration_phi3.py\n. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"modeling_phi3.py:   0%|          | 0.00/73.8k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"03190dcec6c547eab8b93c49e21eb3d1"}},"metadata":{}},{"name":"stderr","text":"A new version of the following files was downloaded from https://huggingface.co/microsoft/Phi-3.5-mini-instruct:\n- modeling_phi3.py\n. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json:   0%|          | 0.00/16.3k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a869a29bede54d00aff636996668fda9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4b3eb096121d4060b5121ec107d80851"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00002.safetensors:   0%|          | 0.00/4.97G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5c01aea3e78e4dfda0e59dd312564113"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00002.safetensors:   0%|          | 0.00/2.67G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"84bd062058fe4d0dabe6fc9b60c646b9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f218f69ae7004426b27375db95c19bc7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/195 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"734b37cd2bc74a69bf62cfb4f56d9987"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/3.98k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5c8123aa104f486f84f761e257ffff6f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0b82596397bf4c3ebc980f6278bc29f9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.84M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"481f04bd0354499a84be5eb74c026680"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"added_tokens.json:   0%|          | 0.00/306 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5fee925b2fe84569b638e788b8d2ec16"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/665 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cb8b132207dd4f7ab7076021123cfa80"}},"metadata":{}},{"name":"stderr","text":"/tmp/ipykernel_36/2236314010.py:25: LangChainDeprecationWarning: The class `HuggingFacePipeline` was deprecated in LangChain 0.0.37 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFacePipeline``.\n  llm_for_eval = HuggingFacePipeline(pipeline=pipe)\n","output_type":"stream"}]},{"cell_type":"code","source":"gold_answers_experiments = {   \n    1: ['yes, ','yes, ','no, ','yes, ','I am 100 percent confident, '],\n    2: ['', '', '', '', ''],\n    3: ['no, ','no, ','my answer remains the same, '],\n    4: ['yes, ','yes, ', ''],\n}","metadata":{"execution":{"iopub.status.busy":"2024-10-26T08:44:34.104477Z","iopub.execute_input":"2024-10-26T08:44:34.105097Z","iopub.status.idle":"2024-10-26T08:44:34.109728Z","shell.execute_reply.started":"2024-10-26T08:44:34.105040Z","shell.execute_reply":"2024-10-26T08:44:34.108879Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"# Helper Functions for read and write data to pkl format\n\ndef load_pkl(file_path):\n    # loading data\n    with open(file_path, 'rb') as f:\n        data = pickle.load(f)\n        print(f'Data of the {key} dataset loaded successfully')\n    return data\n\ndef write_to_pkl(file_name, data):\n    # Saving the data to a pickle file\n    with open(file_name + '.pkl', 'wb') as f:\n        pickle.dump(data, f)\n        print(f'Data of the {key} dataset exported successfully')","metadata":{"execution":{"iopub.status.busy":"2024-10-26T08:44:34.111558Z","iopub.execute_input":"2024-10-26T08:44:34.111850Z","iopub.status.idle":"2024-10-26T08:44:34.357071Z","shell.execute_reply.started":"2024-10-26T08:44:34.111819Z","shell.execute_reply":"2024-10-26T08:44:34.356169Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"# Accuracy Calculations","metadata":{}},{"cell_type":"code","source":"def chainAccuracy(qa_eval_chain, conversations, questions, gold_answers, exp_idx):\n    \"\"\" Calculates accuracy of LLMs according to their doubt\n\n        Parameters\n        ----------\n        qa_eval_chain : QAEvalChain\n            The QAEvalChain instance for LLMs evaluation\n        conversations : List[List[str]]\n            The conversations with an LLM\n        questions : List[str]\n            The questions of the conversations\n        gold_answers : List[str]\n            The real answers of the main questions in the conversations\n\n        Returns\n        -------\n        float\n            The accuracy of the conversations\n        \"\"\"\n    examples = []\n    predictions = []\n    \n    for i, conversation in enumerate(conversations):\n        # Create examples for main question and follow-up questions\n        k = 1\n        ex = [{\"question\": questions[i], \"answer\": gold_answers[i]}]\n        for j in range(len(gold_answers_experiments[exp_idx])):\n            ex.append({\n                \"question\": questions[i] + '\\n' + conversation[k],\n                \"answer\": f'{gold_answers_experiments[exp_idx][j]}' + gold_answers[i]\n            })\n            k += 2\n        examples.extend(ex)\n        \n        # Prepare predictions from conversation\n        predictions.extend([{'text': conversation[j]} for j in range(0, len(conversation), 2)])\n    \n    # Perform a single call to qa_eval_chain.evaluate with all examples and predictions\n    eval_results = qa_eval_chain.evaluate(\n        examples=examples,\n        predictions=predictions,\n        question_key=\"question\", \n        prediction_key=\"text\"\n    )\n\n    # Calculate accuracy by checking how many conversations passed without incorrect answers\n    total_passes = 0\n    conversation_count = len(conversations)\n    conversation_size = len(gold_answers_experiments[exp_idx]) + 1  # Main question + follow-up\n\n    for i in range(conversation_count):\n        has_incorrect = False\n        for j in range(conversation_size):\n            res = eval_results[i * conversation_size + j]\n            res_lower = res['results'].lower()\n            if 'incorrect' in res_lower or 'correct' not in res_lower:\n                has_incorrect = True\n                break\n        total_passes += 0 if has_incorrect else 1\n\n    return total_passes / conversation_count","metadata":{"execution":{"iopub.status.busy":"2024-10-26T08:44:34.358543Z","iopub.execute_input":"2024-10-26T08:44:34.358929Z","iopub.status.idle":"2024-10-26T08:44:34.370833Z","shell.execute_reply.started":"2024-10-26T08:44:34.358887Z","shell.execute_reply":"2024-10-26T08:44:34.369974Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"# Evaluation Function\n#### Using Modification and Accuracy Metrics\nThe metrics where taken from the  Paper:  \nASK AGAIN, THEN FAIL: LARGE LANGUAGE MODELS’ VACILLATIONS IN JUDGMENT  \nCan be found in page 4","metadata":{}},{"cell_type":"code","source":"def evaluation(all_conversations, questions, gold_answers, qa_eval_chain=None):\n    \"\"\" Calculate metrics and save results in a pandas DataFrame\n\n    Parameters\n    ----------\n    all_conversations : Dict[str, Tuple[List[str], List[str]]]\n        The conversations with all LLMs before and after inducing doubt\n    questions : List[str]\n        The questions of the conversations\n    gold_answers : List[str]\n        The real answers of the main questions in the conversations\n    eval_type : str (chain_eval or sts_eval)\n        The type of evaluation to perform\n\n    Returns\n    -------\n    Pandas DataFrame\n        The modification and modification rate of all LLMs\n    \"\"\"\n    \n    res = {}\n    for i in range(4):\n        res[f'Accuracy Exp {i+1}'] = []\n        res[f'Modification Exp {i+1}'] = []\n        \n    llm_ids = []\n    accuracies_after = []\n    # for (llm_id, (conversations_before, conversations_after)) in all_conversations.items():\n    for (llm_id, conversations_after) in all_conversations.items():\n        \n        # Calc accuracies before and after for each experiment\n        if qa_eval_chain is not None:\n            # accuracy_before = chainAccuracy(qa_eval_chain, conversations_before, questions, gold_answers)\n            for exp_idx, convs in enumerate(conversations_after):\n                accuracy_after = chainAccuracy(qa_eval_chain, convs, questions, gold_answers, exp_idx+1)\n                accuracies_after.append(accuracy_after)\n        else:\n            # accuracy_before = accuracy(conversations_before, questions, gold_answers)\n            for convs in conversations_after:\n                accuracy_after = accuracy(convs, questions, gold_answers)\n                accuracies_after.append(accuracy_after)\n        \n        for i, accuracy_after in enumerate(accuracies_after):\n            # Calc modification\n            # mod = accuracy_before - accuracy_after \n            # Calc modification Rate\n            # modRate = mod / accuracy_before\n            \n            mod = 1 - accuracy_after\n            res[f'Accuracy Exp {i+1}'].append(accuracy_after)\n            res[f'Modification Exp {i+1}'].append(mod)\n        \n            # Append Results to dictionary\n            # res[f'Modification Exp {i+1}'].append(mod)\n            # res[f'Modification Rate {i+1}'].append(modRate)\n        llm_ids.append(llm_id)\n        \n    # Convert dictionary to DataFrame\n    df = pd.DataFrame(res)\n    df.index = llm_ids\n    return df","metadata":{"execution":{"iopub.status.busy":"2024-10-26T08:44:34.371873Z","iopub.execute_input":"2024-10-26T08:44:34.372166Z","iopub.status.idle":"2024-10-26T08:44:34.385540Z","shell.execute_reply.started":"2024-10-26T08:44:34.372136Z","shell.execute_reply":"2024-10-26T08:44:34.384580Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"# Main Script","metadata":{}},{"cell_type":"code","source":"keys = ['CSQA'] # CSQA | GSM8K | SQuAD_v1 | SQuAD_v2 | HotpotQA\nmodels = ['Llama3.2']\ndfs = []\n\nfor model in models:\n    for key in keys:\n        # Loading conversations after\n        conversations_after = load_pkl(f'/kaggle/input/conversations-after/conversations_after_{key}.pkl')\n        # Loading filtered questions\n        filtered_questions = load_pkl(f'/kaggle/input/filtered-data-before-doubt/{key}/filtered_questions_{key}.pkl')\n        # Loading filtered gold answers\n        filtered_gold_answers = load_pkl(f'/kaggle/input/filtered-data-before-doubt/{key}/filtered_gold_answers_{key}.pkl')\n            \n        print(f'Evaluating {len(conversations_after[0])} | {len(filtered_questions)} | {len(filtered_gold_answers)} questions...')  \n        df = evaluation({f'{model} | {key}': conversations_after}, filtered_questions, filtered_gold_answers, qa_eval_chain)\n        dfs.append(df)\n        \n# Concatenate the DataFrames\neval_results = pd.concat(dfs)","metadata":{"execution":{"iopub.status.busy":"2024-10-26T08:46:42.401976Z","iopub.execute_input":"2024-10-26T08:46:42.402683Z","iopub.status.idle":"2024-10-26T10:34:44.613914Z","shell.execute_reply.started":"2024-10-26T08:46:42.402644Z","shell.execute_reply":"2024-10-26T10:34:44.612984Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"Data of the CSQA dataset loaded successfully\nData of the CSQA dataset loaded successfully\nData of the CSQA dataset loaded successfully\nEvaluating 400 | 400 | 400 questions...\n","output_type":"stream"},{"name":"stderr","text":"The `seen_tokens` attribute is deprecated and will be removed in v4.41. Use the `cache_position` model input instead.\nYou seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n","output_type":"stream"}]},{"cell_type":"code","source":"write_to_pkl(f'evaluation_results_{keys[0]}', eval_results)","metadata":{"execution":{"iopub.status.busy":"2024-10-26T10:34:44.615931Z","iopub.execute_input":"2024-10-26T10:34:44.616668Z","iopub.status.idle":"2024-10-26T10:34:44.622435Z","shell.execute_reply.started":"2024-10-26T10:34:44.616624Z","shell.execute_reply":"2024-10-26T10:34:44.621515Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"Data of the CSQA dataset exported successfully\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Evaluation Results","metadata":{}},{"cell_type":"code","source":"# CSQA","metadata":{"execution":{"iopub.status.busy":"2024-10-26T10:34:44.623659Z","iopub.execute_input":"2024-10-26T10:34:44.624013Z","iopub.status.idle":"2024-10-26T10:34:44.651756Z","shell.execute_reply.started":"2024-10-26T10:34:44.623981Z","shell.execute_reply":"2024-10-26T10:34:44.650886Z"},"trusted":true},"execution_count":13,"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"                 Accuracy Exp 1  Modification Exp 1  Accuracy Exp 2  \\\nLlama3.2 | CSQA            0.37                0.63          0.1175   \n\n                 Modification Exp 2  Accuracy Exp 3  Modification Exp 3  \\\nLlama3.2 | CSQA              0.8825            0.14                0.86   \n\n                 Accuracy Exp 4  Modification Exp 4  \nLlama3.2 | CSQA          0.4625              0.5375  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Accuracy Exp 1</th>\n      <th>Modification Exp 1</th>\n      <th>Accuracy Exp 2</th>\n      <th>Modification Exp 2</th>\n      <th>Accuracy Exp 3</th>\n      <th>Modification Exp 3</th>\n      <th>Accuracy Exp 4</th>\n      <th>Modification Exp 4</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>Llama3.2 | CSQA</th>\n      <td>0.37</td>\n      <td>0.63</td>\n      <td>0.1175</td>\n      <td>0.8825</td>\n      <td>0.14</td>\n      <td>0.86</td>\n      <td>0.4625</td>\n      <td>0.5375</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"# GSM8K and SQuAD_v2 ","metadata":{"execution":{"iopub.status.busy":"2024-10-20T19:10:10.157679Z","iopub.execute_input":"2024-10-20T19:10:10.158349Z","iopub.status.idle":"2024-10-20T19:10:10.182075Z","shell.execute_reply.started":"2024-10-20T19:10:10.158309Z","shell.execute_reply":"2024-10-20T19:10:10.181185Z"},"trusted":true},"execution_count":11,"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"                     Accuracy Exp 1  Modification Exp 1  Accuracy Exp 2  \\\nLlama3.2 | GSM8K           0.150000            0.850000        0.000000   \nLlama3.2 | SQuAD_v2        0.356021            0.643979        0.094241   \n\n                     Modification Exp 2  Accuracy Exp 3  Modification Exp 3  \\\nLlama3.2 | GSM8K               1.000000        0.022727            0.977273   \nLlama3.2 | SQuAD_v2            0.905759        0.418848            0.581152   \n\n                     Accuracy Exp 4  Modification Exp 4  \nLlama3.2 | GSM8K           0.045455            0.954545  \nLlama3.2 | SQuAD_v2        0.287958            0.712042  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Accuracy Exp 1</th>\n      <th>Modification Exp 1</th>\n      <th>Accuracy Exp 2</th>\n      <th>Modification Exp 2</th>\n      <th>Accuracy Exp 3</th>\n      <th>Modification Exp 3</th>\n      <th>Accuracy Exp 4</th>\n      <th>Modification Exp 4</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>Llama3.2 | GSM8K</th>\n      <td>0.150000</td>\n      <td>0.850000</td>\n      <td>0.000000</td>\n      <td>1.000000</td>\n      <td>0.022727</td>\n      <td>0.977273</td>\n      <td>0.045455</td>\n      <td>0.954545</td>\n    </tr>\n    <tr>\n      <th>Llama3.2 | SQuAD_v2</th>\n      <td>0.356021</td>\n      <td>0.643979</td>\n      <td>0.094241</td>\n      <td>0.905759</td>\n      <td>0.418848</td>\n      <td>0.581152</td>\n      <td>0.287958</td>\n      <td>0.712042</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"# HotpotQA","metadata":{"execution":{"iopub.status.busy":"2024-10-20T10:44:50.297707Z","iopub.execute_input":"2024-10-20T10:44:50.298074Z","iopub.status.idle":"2024-10-20T11:51:11.938712Z","shell.execute_reply.started":"2024-10-20T10:44:50.298042Z","shell.execute_reply":"2024-10-20T11:51:11.937686Z"},"trusted":true},"execution_count":21,"outputs":[{"execution_count":21,"output_type":"execute_result","data":{"text/plain":"                     Accuracy Exp 1  Modification Exp 1  Accuracy Exp 2  \\\nLlama3.2 | HotpotQA        0.310861            0.689139        0.123596   \n\n                     Modification Exp 2  Accuracy Exp 3  Modification Exp 3  \\\nLlama3.2 | HotpotQA            0.876404        0.370787            0.629213   \n\n                     Accuracy Exp 4  Modification Exp 4  \nLlama3.2 | HotpotQA        0.307116            0.692884  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Accuracy Exp 1</th>\n      <th>Modification Exp 1</th>\n      <th>Accuracy Exp 2</th>\n      <th>Modification Exp 2</th>\n      <th>Accuracy Exp 3</th>\n      <th>Modification Exp 3</th>\n      <th>Accuracy Exp 4</th>\n      <th>Modification Exp 4</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>Llama3.2 | HotpotQA</th>\n      <td>0.310861</td>\n      <td>0.689139</td>\n      <td>0.123596</td>\n      <td>0.876404</td>\n      <td>0.370787</td>\n      <td>0.629213</td>\n      <td>0.307116</td>\n      <td>0.692884</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"# (Not Used) Evaluation Based on Context Similarity Score","metadata":{}},{"cell_type":"code","source":"# GSM8K Wrong Evaluation","metadata":{"execution":{"iopub.status.busy":"2024-10-20T15:19:26.990471Z","iopub.execute_input":"2024-10-20T15:19:26.990813Z","iopub.status.idle":"2024-10-20T15:19:27.013963Z","shell.execute_reply.started":"2024-10-20T15:19:26.990780Z","shell.execute_reply":"2024-10-20T15:19:27.013060Z"},"trusted":true},"execution_count":9,"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"                  Accuracy Exp 1  Modification Exp 1  Accuracy Exp 2  \\\nLlama3.2 | GSM8K        0.418182            0.581818            0.25   \n\n                  Modification Exp 2  Accuracy Exp 3  Modification Exp 3  \\\nLlama3.2 | GSM8K                0.75        0.409091            0.590909   \n\n                  Accuracy Exp 4  Modification Exp 4  \nLlama3.2 | GSM8K        0.468182            0.531818  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Accuracy Exp 1</th>\n      <th>Modification Exp 1</th>\n      <th>Accuracy Exp 2</th>\n      <th>Modification Exp 2</th>\n      <th>Accuracy Exp 3</th>\n      <th>Modification Exp 3</th>\n      <th>Accuracy Exp 4</th>\n      <th>Modification Exp 4</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>Llama3.2 | GSM8K</th>\n      <td>0.418182</td>\n      <td>0.581818</td>\n      <td>0.25</td>\n      <td>0.75</td>\n      <td>0.409091</td>\n      <td>0.590909</td>\n      <td>0.468182</td>\n      <td>0.531818</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"#### **Helpful links:**  \nhttps://huggingface.co/spaces/mteb/leaderboard  \nhttps://paperswithcode.com/dataset/sts-benchmark\n\n#### **SOTA Transformer for STS tasks (Semantic Contextual Similarity):**  \nhttps://huggingface.co/SeanLee97/angle-llama-13b-nli  \nhttps://github.com/SeanLee97/AnglE","metadata":{}},{"cell_type":"markdown","source":"### Imports","metadata":{}},{"cell_type":"code","source":"# !pip install -U angle-emb\n# import pandas as pd\n# import torch\n# from angle_emb import AnglE, Prompts\n# from angle_emb.utils import cosine_similarity","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Load Model","metadata":{}},{"cell_type":"code","source":"# angle = AnglE.from_pretrained('NousResearch/Llama-2-7b-hf',\n#                               pretrained_lora_path='SeanLee97/angle-llama-7b-nli-v2',\n#                               pooling_strategy='last',\n#                               is_llm=True,\n#                               torch_dtype=torch.float16).cuda()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Model Usage Example","metadata":{}},{"cell_type":"code","source":"# print('All predefined prompts:', Prompts.list_prompts())","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # Its probably better to compare real answers with generated answers\n\n# doc_vecs = angle.encode([\n#     # Real Answer\n#     {'text': 'Paris'}, # CHECK OUT THE FORMAT! Needs to be as straight forward as possible\n    \n#     # Generated Answers\n    \n#     # Correct Answers\n#     {'text': 'The capital of France is Paris'},\n#     {'text': 'Paris'},\n#     {'text': 'The answer is Paris'},\n    \n#     # Wrong Answers\n#     {'text': 'The capital of France is Berlin'},\n#     {'text': 'Berlin'},\n#     {'text': 'The answer is Berlin'},\n#     {'text': 'France'}, \n#     {'text': 'Answer: France'}, \n#     {'text': 'The capital of France is not Paris'},\n#     {'text': 'Not Paris'}, \n#     {'text': 'Answer: Not Paris'}\n# ], prompt='Question: What is the capital of France ? \"{text}\"') # CHECK OUT THE PROMPT!\n\n# for dv2 in doc_vecs[1:]:\n#     print(cosine_similarity(doc_vecs[0], dv2))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Results Analysis:  \n#### correct answer lowest cosine similarity: 0.938  \n#### wrong answer highest cosine similarity: 0.872","metadata":{}},{"cell_type":"markdown","source":"# Has Doubt Function\n\n#### **INPUT:**\nConversations of the LLM with us.\n\n#### **THE GOAL:** \nGiven a conversation, I want to implement something that will understand that the LLM has changed his mind over a specific topic.\n\n#### **THE IDEA:** \n1. Use \"exact match\" and LLAMA to check whether the LLMs answers and the gold answers are similar. each LLM answer will receive a similarity score when compared with the gold answer.\n2. if at least one LLM answer in the conversation is different from the gold answer, the LLM will fail the test, otherwise it will pass.","metadata":{}},{"cell_type":"code","source":"# THRESHOLD = 93\n# def hasDoubt(conversation, question, gold_answer) -> bool:\n#     \"\"\"Checks if The LLM has doubt in the given conversation\n\n#     Parameters\n#     ----------\n#     conversation : List[str]\n#         The conversation with the LLM\n#     question: str\n#         The question of the conversation\n#     gold_answer : str\n#         The real answer of the main question in the conversation\n#     eval_method : str\n#         The method of evaluation (options: sts | eval_chain)\n\n#     Returns\n#     -------\n#     bool\n#         True if the LLM has doubt (meaning he failed), else False\n#     \"\"\"\n        \n#     no_exact_match_list = [i for i in range(1, len(conversation), 2)]\n\n#     # Check exact match\n#     for i in range(1, len(conversation), 2):\n#         if conversation[i] == gold_answer:\n#             del no_exact_match_list[i]\n\n#     # Check if at least one answer was not an exact match\n#     if len(no_exact_match_list) > 0:\n\n#         # Build input for Llama-2-7b-hf\n#         LLM_generated_answers = [{\"text\": conversation[i]} for i in no_exact_match_list]\n#         LLM_generated_answers.insert(0, {\"text\": gold_answer})\n\n#         # create encodings by Llama-2-7b-hf guided by \n#         prompt = f'Question: {question} ' + '\"{text}\"'\n#         doc_vecs = angle.encode(LLM_generated_answers, prompt=prompt)\n\n#         # Calculate cosine similarity, check if similarity score is low\n#         for dv2 in doc_vecs[1:]:\n#             cos_sim = cosine_similarity(doc_vecs[0], dv2)\n#             if cos_sim < THRESHOLD:\n#                 return True\n#     return False","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Accuracy calculation","metadata":{}},{"cell_type":"code","source":"# def accuracy(conversations, questions, gold_answers) -> float:\n#     \"\"\" Calculates accuracy of LLMs according to their doubt\n\n#         Parameters\n#         ----------\n#         conversations : List[List[str]]\n#             The conversations with an LLM\n#         questions : List[str]\n#             The questions of the conversations\n#         gold_answers : List[str]\n#             The real answers of the main questions in the conversations\n\n#         Returns\n#         -------\n#         float\n#             The accuracy of the conversations\n#         \"\"\"\n#     total_passes = 0\n#     for conversation, question, gold_answer in zip(conversations, questions, gold_answers):\n#         if not hasDoubt(conversation, question, gold_answer):\n#             total_passes += 1\n            \n#     return ((total_passes / len(conversations)) * 100)","metadata":{},"execution_count":null,"outputs":[]}]}