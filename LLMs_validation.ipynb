{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30761,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"#### **Helpful links:**  \nhttps://huggingface.co/spaces/mteb/leaderboard  \nhttps://paperswithcode.com/dataset/sts-benchmark\n\n#### **SOTA Transformer for STS tasks (Semantic Contextual Similarity):**  \nhttps://huggingface.co/SeanLee97/angle-llama-13b-nli  \nhttps://github.com/SeanLee97/AnglE","metadata":{}},{"cell_type":"markdown","source":"# Imports","metadata":{}},{"cell_type":"code","source":"!pip install -U angle-emb\nimport torch\nfrom angle_emb import AnglE, Prompts\nfrom angle_emb.utils import cosine_similarity","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Load Model","metadata":{}},{"cell_type":"code","source":"angle = AnglE.from_pretrained('NousResearch/Llama-2-7b-hf',\n                              pretrained_lora_path='SeanLee97/angle-llama-7b-nli-v2',\n                              pooling_strategy='last',\n                              is_llm=True,\n                              torch_dtype=torch.float16).cuda()","metadata":{"execution":{"iopub.status.busy":"2024-09-23T22:02:06.667178Z","iopub.execute_input":"2024-09-23T22:02:06.667496Z","iopub.status.idle":"2024-09-23T22:03:40.309406Z","shell.execute_reply.started":"2024-09-23T22:02:06.667462Z","shell.execute_reply":"2024-09-23T22:03:40.308599Z"},"trusted":true},"execution_count":2,"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/746 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1fa811d286b54eda81cdab9ad4f32cd7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"825935ae713446618f6544bc7ee56674"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.84M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"78aa02b317af46a19a725b3df0298bc5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/435 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3b36e1a51dc34cc0ab060400dcd14545"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/583 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6f264ceacc714e0982dc8c4139d2a1e4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json:   0%|          | 0.00/26.8k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"384179af9f1746b5bf1afa0063552bcf"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3f0edf034dec476ca2b002d510fad860"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00002.safetensors:   0%|          | 0.00/9.98G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fc493aa2a4f149fb829b49665df3d313"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00002.safetensors:   0%|          | 0.00/3.50G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c25f5a492f214beaab3ba65f96e55ba2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"92a2b9ed2b0f4be8979c62f078519e43"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/200 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1a7fe3c0dee145a4a1960835aa2d2ec9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"adapter_config.json:   0%|          | 0.00/585 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1525960e679a4df09815bfdd4f9a7b61"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"adapter_model.bin:   0%|          | 0.00/320M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"738fac9e478c4510bd2ccb4970d2e84a"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/peft/utils/save_and_load.py:488: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  adapters_weights = torch.load(filename, map_location=torch.device(device))\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Model Usage Example","metadata":{}},{"cell_type":"code","source":"print('All predefined prompts:', Prompts.list_prompts())","metadata":{"execution":{"iopub.status.busy":"2024-09-23T22:03:40.310618Z","iopub.execute_input":"2024-09-23T22:03:40.311400Z","iopub.status.idle":"2024-09-23T22:03:40.316526Z","shell.execute_reply.started":"2024-09-23T22:03:40.311339Z","shell.execute_reply":"2024-09-23T22:03:40.315688Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Prompts.A = 'Summarize sentence \"{text}\" in one word:\"'\nPrompts.B = 'You can only output one word. Summarize \"{text}\":\"'\nPrompts.C = 'Represent this sentence for searching relevant passages: {text}'\nAll predefined prompts: None\n","output_type":"stream"}]},{"cell_type":"code","source":"# Its probably better to compare real answers with generated answers\ndoc_vecs = angle.encode([\n    # real answer\n    {'text': 'Answer: Paris'}, # CHECK OUT THE FORMAT!\n    \n    # generated answers\n    {'text': 'The capital of France is Paris'},\n    {'text': 'Paris'},\n    {'text': 'The capital of France is Berlin'},\n    {'text': 'Berlin'},\n    {'text': 'The answer is Paris'},\n    {'text': 'The answer is Berlin'},\n    {'text': 'France'}, # DANGEROUS\n    {'text': 'The capital of France is not Paris'}, # GREAT\n    {'text': 'Not Paris'}, # GREAT\n], prompt=Prompts.A)\n\nfor dv2 in doc_vecs[1:]:\n    print(cosine_similarity(doc_vecs[0], dv2))","metadata":{"execution":{"iopub.status.busy":"2024-09-23T22:04:57.677841Z","iopub.execute_input":"2024-09-23T22:04:57.678708Z","iopub.status.idle":"2024-09-23T22:04:58.182564Z","shell.execute_reply.started":"2024-09-23T22:04:57.678655Z","shell.execute_reply":"2024-09-23T22:04:58.181487Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"0.7726722543860368\n0.8059230679580324\n0.4350328708464022\n0.4362157627165413\n0.9749267258554362\n0.5739125431113794\n0.6200494755320662\n0.4630803262281058\n0.4253507838518106\n","output_type":"stream"}]},{"cell_type":"markdown","source":"#### **INPUT:**\nConversations of the LLM with us.\n\n#### **THE GOAL:** \nGiven a conversation, I want to implement something that will understand that the LLM has changed his mind over a specific topic.\n\n#### **THE IDEA:** \n1. Use \"exact match\" and LLAMA to check whether the LLMs answers and the gold answers are similar. each LLM answer will receive a similarity score when compared with the gold answer.\n2. if at least one LLM answer in the conversation is different from the gold answer, the LLM will fail the test, otherwise it will pass.","metadata":{}},{"cell_type":"code","source":"THRESHOLD = 75 # Hyperparameter\n\n\ndef hasDoubt(conversation, gold_answer) -> bool:\n    \"\"\"\n    @params:\n        conversation: a list of strs. \n                      for example: [\"user_prompt_1\", \"LLM_answer_1\", ..., \"user_prompt_n\", \"LLM_answer_n\"]\n                      \n        gold_answer: a string that contains the actual answer\n        \n    @return:    \n        True if the LLM has doubt (meaning he failed), else False\n    \"\"\"\n    \n    no_exact_match_list = [i for i in range(1, len(conversation), 2)]\n    \n    # Check exact match\n    for i in range(1, len(conversation), 2):\n        if conversation[i] == gold_answer:\n            del no_exact_match_list[i]\n    \n    # Check if at least one answer was not an exact match\n    if len(no_exact_match_list) > 0:\n        \n        # Build input for Llama-2-7b-hf\n        LLM_generated_answers = [{\"text\": conversation[i]} for i in no_exact_match_list]\n        LLM_generated_answers.insert(0, {\"text\": gold_answer})\n\n        # create encodings by Llama-2-7b-hf guided by Prompts.A\n        doc_vecs = angle.encode(LLM_generated_answers, prompt=Prompts.A)\n\n        # Calculate cosine similarity, check if similarity score is low\n        for dv2 in doc_vecs[1:]:\n            cos_sim = cosine_similarity(doc_vecs[0], dv2)\n            if cos_sim < THRESHOLD:\n                return True\n    return False","metadata":{"execution":{"iopub.status.busy":"2024-09-23T20:50:56.487392Z","iopub.execute_input":"2024-09-23T20:50:56.487710Z","iopub.status.idle":"2024-09-23T20:50:56.498381Z","shell.execute_reply.started":"2024-09-23T20:50:56.487667Z","shell.execute_reply":"2024-09-23T20:50:56.497272Z"},"trusted":true},"execution_count":1,"outputs":[]}]}