{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":9726491,"sourceType":"datasetVersion","datasetId":5889639},{"sourceId":9726641,"sourceType":"datasetVersion","datasetId":5881217},{"sourceId":9766508,"sourceType":"datasetVersion","datasetId":5879652},{"sourceId":201479431,"sourceType":"kernelVersion"},{"sourceId":204365681,"sourceType":"kernelVersion"}],"dockerImageVersionId":30786,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from global_utils import CFG, load_pkl, write_to_pkl, load_eval_llm\nfrom datasets_manipulations import load_datasets\n\nfrom langchain_community.chat_message_histories import SQLChatMessageHistory\nfrom langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\nfrom langchain_core.runnables.history import RunnableWithMessageHistory\nfrom langchain.llms import HuggingFaceEndpoint, HuggingFacePipeline\nfrom langchain_core.messages import HumanMessage, BaseMessage\nfrom langchain.evaluation.qa import QAEvalChain\n\nfrom transformers import AutoTokenizer, pipeline\nfrom huggingface_hub import login\n\nfrom datasets import Dataset\nimport accelerate\nimport warnings\nimport sqlite3\nimport pickle\nimport shutil\nimport torch\nimport re\nimport os\n\nlogin(token=CFG.credentials['llama3.2'])\nwarnings.filterwarnings('ignore')","metadata":{"execution":{"iopub.status.busy":"2024-10-31T12:01:43.565285Z","iopub.execute_input":"2024-10-31T12:01:43.565837Z","iopub.status.idle":"2024-10-31T12:01:43.652347Z","shell.execute_reply.started":"2024-10-31T12:01:43.565798Z","shell.execute_reply":"2024-10-31T12:01:43.651453Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\nToken is valid (permission: fineGrained).\nYour token has been saved to /root/.cache/huggingface/token\nLogin successful\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Load Datasets","metadata":{}},{"cell_type":"code","source":"datasets_to_load = CFG.supported_datasets # CSQA | GSM8K | SQuAD_v1 | SQuAD_v2 | HotpotQA\nqa_lists = load_datasets(datasets_to_load)\n\nall_questions = {}\nall_gold_answers = {}\nall_examples = {}\nall_configs = {}\n\nfor key in datasets_to_load:\n    qa = qa_lists[key]\n    questions = [entry['question'] for entry in qa[:CFG.n]]\n    gold_answers = [entry['correct_answer'] for entry in qa[:CFG.n]]\n    examples = [{\"question\": q} for q in questions]\n    configs = [{\"configurable\": {\"session_id\": f\"{i+1}\"}} for i in range(len(examples))]\n    \n    all_questions[key] = questions\n    all_gold_answers[key] = gold_answers\n    all_examples[key] = examples\n    all_configs[key] = configs","metadata":{"execution":{"iopub.status.busy":"2024-10-31T12:01:43.653365Z","iopub.execute_input":"2024-10-31T12:01:43.653651Z","iopub.status.idle":"2024-10-31T12:01:48.348762Z","shell.execute_reply.started":"2024-10-31T12:01:43.653621Z","shell.execute_reply":"2024-10-31T12:01:48.347751Z"},"trusted":true},"execution_count":3,"outputs":[{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/7.94k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8f915e0574784d399ed1a214eb7b9e39"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train-00000-of-00001.parquet:   0%|          | 0.00/2.31M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f66b2bf545c54ec5bae5d01b579a55d0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"test-00000-of-00001.parquet:   0%|          | 0.00/419k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c9386f91bd5a4afd9fca2d9cfcb0619f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/7473 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0d9407e38fdd452c81b0070996844859"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/1319 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bbf837cf03d9484888ff73d11d511c48"}},"metadata":{}}]},{"cell_type":"markdown","source":"# Define LLMs","metadata":{}},{"cell_type":"markdown","source":"## Llama 3.2","metadata":{}},{"cell_type":"code","source":"tokenizer=AutoTokenizer.from_pretrained(CFG.model)\n\n# Check if pad_token_id is missing, and set it to eos_token_id if needed\nif tokenizer.pad_token_id is None:\n    tokenizer.pad_token_id = tokenizer.eos_token_id\n    \npl = pipeline(\n    \"text-generation\",\n    model=CFG.model,\n    tokenizer=tokenizer,\n    return_full_text=False,\n    torch_dtype=torch.bfloat16,\n    trust_remote_code=True,\n    device_map=\"auto\",\n    no_repeat_ngram_size=3,\n    max_new_tokens=150,\n    do_sample=False,\n    num_return_sequences=1,\n    eos_token_id=tokenizer.eos_token_id,\n    repetition_penalty=1.1,\n    )\n\nllm = HuggingFacePipeline(pipeline=pl)","metadata":{"execution":{"iopub.status.busy":"2024-10-31T12:01:48.351555Z","iopub.execute_input":"2024-10-31T12:01:48.351994Z","iopub.status.idle":"2024-10-31T12:04:26.567464Z","shell.execute_reply.started":"2024-10-31T12:01:48.351944Z","shell.execute_reply":"2024-10-31T12:04:26.566503Z"},"trusted":true},"execution_count":4,"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/54.5k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9c734b8187574501928b5f480170f2f3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c946481231fb46efb17d9dff9591c7e2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/296 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fc6066df3baa4dfb96aac0ff601ce06b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/878 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1a891b4ed1f34cca8f6b4dbff00ba6b0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json:   0%|          | 0.00/20.9k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2c37e2d68372424ab30a2b7a03141924"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"26fff4bcc4ea4f949445697203c9329b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00002.safetensors:   0%|          | 0.00/4.97G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"49bd8e1b8e7c40d78aaa39a247683f27"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00002.safetensors:   0%|          | 0.00/1.46G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"241351c2af0d4803a6d73355da503c3c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"34a620b4912e49d98b3b6ecdc73207c5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/189 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9d8032eafe214b8da4416b33d5de06e7"}},"metadata":{}}]},{"cell_type":"markdown","source":"### Define DB Functions","metadata":{}},{"cell_type":"code","source":"def get_session_history(session_id):\n    return SQLChatMessageHistory(session_id, \"sqlite:///memory.db\")\n\ndef update_db(human_messages, ai_messages):\n    # Check if a memory database already exists.\n    file_path = f'/kaggle/input/filtered-data-before-doubt/{key}/memory.db'\n    if os.path.exists(file_path):\n        # Load memory.db to working folder\n        shutil.copy(file_path, '/kaggle/working/')\n        delete_except_first_two()\n    else:\n        for session_id, question in enumerate(human_messages):\n            db = get_session_history(f'{session_id+1}')\n            db.add_messages([BaseMessage(content=question, type='human'),\n                             BaseMessage(content=ai_messages[session_id]['text'], type='ai')])\n    \ndef delete_except_first_two():\n    # Connect to the SQLite database\n    conn = sqlite3.connect('memory.db')\n    cursor = conn.cursor()\n    \n    # Step 1: Identify the message ids to delete (rank > 2 per session)\n    cursor.execute(\"\"\"\n        WITH ranked_messages AS (\n          SELECT\n            id,\n            ROW_NUMBER() OVER (PARTITION BY session_id ORDER BY id ASC) AS rn\n          FROM message_store\n        )\n        SELECT id\n        FROM ranked_messages\n        WHERE rn > 2;\n    \"\"\")\n    \n    ids_to_delete = cursor.fetchall()\n    \n    if ids_to_delete:\n        # Step 2: Execute the DELETE statement for all ids except the first two\n        cursor.executemany(\"\"\"\n            DELETE FROM message_store\n            WHERE id = ?;\n        \"\"\", [(row[0],) for row in ids_to_delete])\n        \n        conn.commit()\n        print(f\"Deleted {cursor.rowcount} messages.\")\n    else:\n        print(\"No messages to delete.\")\n    \n    # Close the connection\n    conn.close()","metadata":{"execution":{"iopub.status.busy":"2024-10-31T12:04:26.568672Z","iopub.execute_input":"2024-10-31T12:04:26.569027Z","iopub.status.idle":"2024-10-31T12:04:26.578455Z","shell.execute_reply.started":"2024-10-31T12:04:26.568989Z","shell.execute_reply":"2024-10-31T12:04:26.577633Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"### Define Chain","metadata":{}},{"cell_type":"code","source":"def get_llm_chain(key):\n    prefix = CFG.prefixes_map[key]\n    assert prefix != None, CFG.error_messages['prefix'].format(key=key)\n    \n    prompt = ChatPromptTemplate.from_messages(\n        [\n            (\"system\", prefix),\n            MessagesPlaceholder(variable_name=\"history\"),\n            (\"human\", \"{question}\"),\n        ]\n    )\n\n    runnable = prompt | llm\n\n    chain = RunnableWithMessageHistory(\n        runnable,\n        get_session_history,\n        input_messages_key=\"question\",\n        history_messages_key=\"history\",\n    )\n    \n    return chain","metadata":{"execution":{"iopub.status.busy":"2024-10-31T12:04:26.579410Z","iopub.execute_input":"2024-10-31T12:04:26.579683Z","iopub.status.idle":"2024-10-31T12:04:26.594934Z","shell.execute_reply.started":"2024-10-31T12:04:26.579653Z","shell.execute_reply":"2024-10-31T12:04:26.594015Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"# LLM Intergration","metadata":{}},{"cell_type":"markdown","source":"## Get LLM Answer","metadata":{}},{"cell_type":"code","source":"def get_answer(llm, questions, configs):\n    predictions = llm.batch(\n        questions,\n        config=configs,\n    )\n    return [{'text': pred} for pred in predictions]","metadata":{"execution":{"iopub.status.busy":"2024-10-31T12:04:26.595976Z","iopub.execute_input":"2024-10-31T12:04:26.596296Z","iopub.status.idle":"2024-10-31T12:04:26.608539Z","shell.execute_reply.started":"2024-10-31T12:04:26.596263Z","shell.execute_reply":"2024-10-31T12:04:26.607747Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"## Conversations Before Doubt     ","metadata":{}},{"cell_type":"code","source":"# Conversations Before Doubt     \ndef get_conversations_before(key, chain, questions, examples, configs):\n    file_path = f'/kaggle/input/conversations-before/conversations_before_{key}.pkl'\n    if not os.path.exists(file_path):\n        conversations_before = get_answer(chain, examples, configs)\n    else:\n        # Loading conversations_before\n        conversations_before = load_pkl(file_path)\n        if not os.path.exists('/kaggle/working/memory.db'):\n            update_db(questions, conversations_before)\n    return conversations_before","metadata":{"execution":{"iopub.status.busy":"2024-10-31T12:04:26.609518Z","iopub.execute_input":"2024-10-31T12:04:26.609827Z","iopub.status.idle":"2024-10-31T12:04:26.620374Z","shell.execute_reply.started":"2024-10-31T12:04:26.609796Z","shell.execute_reply":"2024-10-31T12:04:26.619519Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"## Extract Only Correct Answers  ","metadata":{}},{"cell_type":"markdown","source":"### Evaluate conversations before  ","metadata":{}},{"cell_type":"code","source":"def evaluate(key, questions, gold_answers):\n    # Initialize QAEvalChain\n    qa_eval_chain = load_eval_llm()\n\n    # Prepare examples (questions with gold answers)\n    if key == 'GSM8K':\n        examples_test = [ {\"question\": q, \"answer\": r.split('#### ')[-1]} for q, r in zip(questions, gold_answers)]\n    else:\n        examples_test = [ {\"question\": q, \"answer\": r} for q, r in zip(questions, gold_answers)]\n\n    # Convert to Datasets objects to improve efficiency\n    examples_test = Dataset.from_list(examples_test)\n    conversations_before_test = Dataset.from_list(conversations_before)\n\n    # Evaluate the model-generated answers by passing 'predictions' separately\n    eval_results = qa_eval_chain.evaluate(examples=examples_test,\n                                          predictions=conversations_before_test,\n                                          question_key=\"question\",\n                                          prediction_key=\"text\")\n\n    return eval_results","metadata":{"execution":{"iopub.status.busy":"2024-10-31T12:04:26.621537Z","iopub.execute_input":"2024-10-31T12:04:26.622150Z","iopub.status.idle":"2024-10-31T12:04:26.631711Z","shell.execute_reply.started":"2024-10-31T12:04:26.622107Z","shell.execute_reply":"2024-10-31T12:04:26.630865Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"### Filter Incorrect Responses","metadata":{}},{"cell_type":"code","source":"def filter_data(key, conversations_before, eval_results, questions, gold_answers, configs):\n    \n    file_path = f'/kaggle/input/filtered-data-before-doubt/{key}'\n    filtered_configs = []\n    if not os.path.exists(file_path):\n        filtered_conversations_before = []\n        filtered_questions = []\n        filtered_gold_answers = []\n        for conv, res, q, a, conf in zip(conversations_before, eval_results, questions, gold_answers, configs):\n            temp = res['results'].lower()\n            if 'correct' in temp and 'incorrect' not in temp:\n                filtered_conversations_before.append(conv)\n                filtered_questions.append(q)\n                filtered_gold_answers.append(a)\n                filtered_configs.append(conf)\n\n    else:\n        # Loading filtered conversations_before\n        filtered_conversations_before = load_pkl(f'/kaggle/input/filtered-data-before-doubt/{key}/filtered_conversations_before_{key}.pkl')\n        # Loading filtered questions\n        filtered_questions = load_pkl(f'/kaggle/input/filtered-data-before-doubt/{key}/filtered_questions_{key}.pkl')\n        # Loading filtered gold answers\n        filtered_gold_answers = load_pkl(f'/kaggle/input/filtered-data-before-doubt/{key}/filtered_gold_answers_{key}.pkl')\n\n        filtered_questions_set = set(filtered_questions)\n        for session_id in range(1, CFG.n+1):\n            q = (get_session_history(f'{session_id}').get_messages()[0]).content\n            if q in filtered_questions_set:\n                filtered_configs.append({\"configurable\": {\"session_id\": f\"{session_id}\"}})\n\n    # build filtered examples\n    filtered_examples = [{\"question\": q} for q in filtered_questions]\n    return filtered_conversations_before, filtered_questions, filtered_gold_answers, filtered_configs, filtered_examples","metadata":{"execution":{"iopub.status.busy":"2024-10-31T12:04:26.634736Z","iopub.execute_input":"2024-10-31T12:04:26.635109Z","iopub.status.idle":"2024-10-31T12:04:26.646618Z","shell.execute_reply.started":"2024-10-31T12:04:26.635078Z","shell.execute_reply":"2024-10-31T12:04:26.645812Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"## Conversations After Doubt","metadata":{}},{"cell_type":"code","source":"def get_conversation_after_doubt(llm, configs, experiment, questions, conversations_before=None):\n    \n    def update_history(_llm, _questions, _configs, history):\n        qs = Dataset.from_list(_questions)\n        preds = get_answer(_llm, qs, _configs)\n        for i, pred in enumerate(preds):\n            history[i].append(pred['text'])\n            \n    if conversations_before is None:\n        history = [[] for _ in range(len(questions))] # idx i: history of question i\n        update_history(llm, questions, configs, history)\n    else:\n        history = [[ans['text']] for ans in conversations_before]\n    \n    for idx, induced_doubt in enumerate(experiment):\n        print(f\"Generateing answers for induced doubt question {idx+1}/{len(experiment)}\")\n        induced_doubts = []\n        for hist in history:\n            hist.append(induced_doubt)\n            induced_doubts.append({\"question\": induced_doubt})\n        update_history(llm, induced_doubts, configs, history)\n    return history","metadata":{"execution":{"iopub.status.busy":"2024-10-31T12:21:50.190169Z","iopub.execute_input":"2024-10-31T12:21:50.191252Z","iopub.status.idle":"2024-10-31T12:21:50.204503Z","shell.execute_reply.started":"2024-10-31T12:21:50.191206Z","shell.execute_reply":"2024-10-31T12:21:50.202141Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"markdown","source":"## Main","metadata":{}},{"cell_type":"code","source":"all_conversations = {}\n\nfor key in datasets_to_load:\n    if key == 'SQuAD_v1':\n        continue\n    print(key)\n    questions = all_questions[key]\n    gold_answers = all_gold_answers[key]\n    examples = all_examples[key]\n    configs = all_configs[key]\n    \n    # Define chain\n    chain = get_llm_chain(key)\n    \n    # Get conversations before and memory database\n    conversations_before = get_conversations_before(key, chain, questions, examples, configs)\n    \n    # Evaluate results and receive filtering initial incorrect responses\n    eval_results = None\n    if not os.path.exists(f'/kaggle/input/filtered-data-before-doubt/{key}'):\n        eval_results = evaluate(key, questions, gold_answers)\n    \n    # Filter incorrect responses \n    data = filter_data(key, conversations_before, eval_results, questions, gold_answers, configs)\n    filtered_conversations_before, filtered_questions, filtered_gold_answers, filtered_configs, filtered_examples = data\n    \n    # Sanity check \n    print('Sanity Check - should print the same lengths:')\n    print(len(filtered_questions), len(filtered_gold_answers), len(filtered_configs))\n    \n    \n    # Get conversations after doubt\n    file_path = f'/kaggle/input/conversations-after/conversations_after_{key}.pkl'\n    if os.path.exists(file_path):\n        conversations_after = []\n        for idx, exp in enumerate(CFG.experiments):\n            print(f'Experiment {idx+1}/{len(CFG.experiments)}')\n            if key == 'GSM8K':\n                filtered_conversations_before = filtered_conversations_before[:220]\n            conversations_after.append(get_conversation_after_doubt(chain, filtered_configs, exp, filtered_questions, filtered_conversations_before))\n            # delete experiment history from all sessions expect for the main question and first response\n            delete_except_first_two() \n    else:\n        # Loading the data from a pickle file\n        conversations_after = load_pkl(file_path)\n    \n    # Delete memory database\n    !rm -rf memory.db\n    \n    all_conversations[key] = [filtered_conversations_before, conversations_after, filtered_questions, filtered_gold_answers]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Save Data For Evaluation","metadata":{}},{"cell_type":"code","source":"for key in datasets_to_load:\n    if key == 'SQuAD_v1':\n        continue\n    write_to_pkl(f'filtered_conversations_before_{key}', all_conversations[key][0])\n    write_to_pkl(f'conversations_after_{key}', all_conversations[key][1])\n    write_to_pkl(f'filtered_questions_{key}', all_conversations[key][2])\n    write_to_pkl(f'filtered_gold_answers_{key}', all_conversations[key][3])\nprint('Done')","metadata":{"execution":{"iopub.status.busy":"2024-10-31T20:57:18.856752Z","iopub.execute_input":"2024-10-31T20:57:18.857651Z","iopub.status.idle":"2024-10-31T20:57:18.870941Z","shell.execute_reply.started":"2024-10-31T20:57:18.857606Z","shell.execute_reply":"2024-10-31T20:57:18.869910Z"},"trusted":true},"execution_count":30,"outputs":[{"name":"stdout","text":"Done\n","output_type":"stream"}]}]}