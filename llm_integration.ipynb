{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":9726491,"sourceType":"datasetVersion","datasetId":5889639},{"sourceId":9726641,"sourceType":"datasetVersion","datasetId":5881217},{"sourceId":9766508,"sourceType":"datasetVersion","datasetId":5879652},{"sourceId":201479431,"sourceType":"kernelVersion"},{"sourceId":204365681,"sourceType":"kernelVersion"}],"dockerImageVersionId":30786,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from global_utils import CFG, load_pkl, write_to_pkl, load_eval_llm\nfrom datasets_manipulations import load_datasets\n\nfrom langchain_community.chat_message_histories import SQLChatMessageHistory\nfrom langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\nfrom langchain_core.runnables.history import RunnableWithMessageHistory\nfrom langchain.llms import HuggingFaceEndpoint, HuggingFacePipeline\nfrom langchain_core.messages import HumanMessage, BaseMessage\nfrom langchain.evaluation.qa import QAEvalChain\n\nfrom transformers import AutoTokenizer, pipeline\nfrom huggingface_hub import login\n\nfrom datasets import Dataset\nimport accelerate\nimport warnings\nimport sqlite3\nimport pickle\nimport shutil\nimport torch\nimport re\nimport os\n\n# NOTE: Add token as enviorement variable\nlogin(token=CFG.credentials['llama3.2'])\nwarnings.filterwarnings('ignore')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Load Datasets","metadata":{}},{"cell_type":"code","source":"datasets_to_load = CFG.supported_datasets # CSQA | GSM8K | SQuAD_v1 | SQuAD_v2 | HotpotQA\nqa_lists = load_datasets(datasets_to_load)\n\nall_questions = {}\nall_gold_answers = {}\nall_examples = {}\nall_configs = {}\n\nfor key in datasets_to_load:\n    qa = qa_lists[key]\n    questions = [entry['question'] for entry in qa[:CFG.n]]\n    gold_answers = [entry['correct_answer'] for entry in qa[:CFG.n]]\n    examples = [{\"question\": q} for q in questions]\n    configs = [{\"configurable\": {\"session_id\": f\"{i+1}\"}} for i in range(len(examples))]\n    \n    all_questions[key] = questions\n    all_gold_answers[key] = gold_answers\n    all_examples[key] = examples\n    all_configs[key] = configs","metadata":{"execution":{"iopub.status.busy":"2024-10-30T19:31:47.304891Z","iopub.execute_input":"2024-10-30T19:31:47.306328Z","iopub.status.idle":"2024-10-30T19:34:31.005788Z","shell.execute_reply.started":"2024-10-30T19:31:47.306269Z","shell.execute_reply":"2024-10-30T19:34:31.004570Z"},"trusted":true},"execution_count":2,"outputs":[{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/7.39k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bfead0a015af40b580557e02051dc8ca"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train-00000-of-00001.parquet:   0%|          | 0.00/1.25M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"96ff0185c3a447f39b6d63b9711a9320"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"validation-00000-of-00001.parquet:   0%|          | 0.00/160k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"374f09962037446ca6107c28b786cfef"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"test-00000-of-00001.parquet:   0%|          | 0.00/151k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3abd926fc54e4f5fb988bdc06c3e17b5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/9741 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"71d2c240e73048299fc33a0f6696570f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split:   0%|          | 0/1221 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d70eb09f41064f4789c94581dae28770"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/1140 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"30813117dd53403585ddb2d0c2c6e11c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/7.94k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f8f9ea56bff04957aca02cb01b7920e7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train-00000-of-00001.parquet:   0%|          | 0.00/2.31M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"596de4b2c4644bcc96bb917354193591"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"test-00000-of-00001.parquet:   0%|          | 0.00/419k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"985ff13f14dc4e07ad2047dd9a644f62"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/7473 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f8ceb15f8b1b45a2a838074d130ff46a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/1319 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"74df6394d8d94c2ea8af8a0c59a46c10"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/7.62k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"009d32210a3241c8a871d897e18c2932"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train-00000-of-00001.parquet:   0%|          | 0.00/14.5M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"42a182c453f74f3bb9ca03a196726b99"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"validation-00000-of-00001.parquet:   0%|          | 0.00/1.82M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0e6ed8e9ad5a48c194b4b2007a20c79a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/87599 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d79f9955369e4db88cf84c269b0918a3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split:   0%|          | 0/10570 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"013d8ffed3a84955becce5b60d20014d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/8.92k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ee26522009544fd6926c7568a8864bff"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train-00000-of-00001.parquet:   0%|          | 0.00/16.4M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"67d8641faba848d996fe1d2246e5c80d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"validation-00000-of-00001.parquet:   0%|          | 0.00/1.35M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3d05654f1631406cad14cdb086ea3ce7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/130319 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1b073caaa4d94736b874775e1a85894a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split:   0%|          | 0/11873 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"20f457d5ae714ebc9956a58cec8134cd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"hotpot_qa.py:   0%|          | 0.00/6.42k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"15dbdb8cebd346d1ab24d40a98f280b5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/9.19k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"82a6545845ce4fb7a64073afbe49393f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/566M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ebc155f0bf6b47f9b36bfbd4c7ef0cd9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/46.3M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3e4d0d44adbe426887d0b53d2d1f9c07"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/90447 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8df3dc20c4024071816b40facbe7ce3a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split:   0%|          | 0/7405 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"304f369f44db4b8d89ea4c77d0dba682"}},"metadata":{}}]},{"cell_type":"markdown","source":"# Define LLMs","metadata":{}},{"cell_type":"markdown","source":"## Llama 3.2","metadata":{}},{"cell_type":"code","source":"tokenizer=AutoTokenizer.from_pretrained(CFG.model)\n\n# Check if pad_token_id is missing, and set it to eos_token_id if needed\nif tokenizer.pad_token_id is None:\n    tokenizer.pad_token_id = tokenizer.eos_token_id\n    \npl = pipeline(\n    \"text-generation\",\n    model=CFG.model,\n    tokenizer=tokenizer,\n    return_full_text=False,\n    torch_dtype=torch.bfloat16,\n    trust_remote_code=True,\n    device_map=\"auto\",\n    no_repeat_ngram_size=3,\n    max_new_tokens=150,\n    do_sample=False,\n    num_return_sequences=1,\n    eos_token_id=tokenizer.eos_token_id,\n    repetition_penalty=1.1,\n    )\n\nllm = HuggingFacePipeline(pipeline=pl)","metadata":{"execution":{"iopub.status.busy":"2024-10-30T19:34:31.007974Z","iopub.execute_input":"2024-10-30T19:34:31.008439Z","iopub.status.idle":"2024-10-30T19:37:08.209685Z","shell.execute_reply.started":"2024-10-30T19:34:31.008388Z","shell.execute_reply":"2024-10-30T19:37:08.208455Z"},"trusted":true},"execution_count":3,"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/54.5k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bc4570a97ee44dcc89ad1b6b395e0ebe"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c9f878a16b2f4eebb0c2c2e9f11c38ba"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/296 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e50fe86dc4674f118126591f84dbcfa3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/878 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e7f41673c1b44463a0ec14cba8d355a1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json:   0%|          | 0.00/20.9k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"56d085283cde4fc2ac43a57cc0102698"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6204aee8963c428aaefdf62b850823df"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00002.safetensors:   0%|          | 0.00/4.97G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8cb7c09dbdeb48f59411ffa08283a239"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00002.safetensors:   0%|          | 0.00/1.46G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2cd1de46909446018d297cfd59476618"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a3749bbf8fbf438bbee968840e30395a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/189 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e723625119a2460f827c3756467747b8"}},"metadata":{}}]},{"cell_type":"markdown","source":"### Define DB Functions","metadata":{}},{"cell_type":"code","source":"def get_session_history(session_id):\n    return SQLChatMessageHistory(session_id, \"sqlite:///memory.db\")\n\ndef update_db(human_messages, ai_messages):\n    # Check if a memory database already exists.\n    file_path = f'/kaggle/input/filtered-data-before-doubt/{key}/memory.db'\n    if os.path.exists(file_path):\n        # Load memory.db to working folder\n        shutil.copy(file_path, '/kaggle/working/')\n        delete_except_first_two()\n    else:\n        for session_id, question in enumerate(human_messages):\n            db = get_session_history(f'{session_id+1}')\n            db.add_messages([BaseMessage(content=question, type='human'),\n                             BaseMessage(content=ai_messages[session_id]['text'], type='ai')])\n    \ndef delete_except_first_two():\n    # Connect to the SQLite database\n    conn = sqlite3.connect('memory.db')\n    cursor = conn.cursor()\n    \n    # Step 1: Identify the message ids to delete (rank > 2 per session)\n    cursor.execute(\"\"\"\n        WITH ranked_messages AS (\n          SELECT\n            id,\n            ROW_NUMBER() OVER (PARTITION BY session_id ORDER BY id ASC) AS rn\n          FROM message_store\n        )\n        SELECT id\n        FROM ranked_messages\n        WHERE rn > 2;\n    \"\"\")\n    \n    ids_to_delete = cursor.fetchall()\n    \n    if ids_to_delete:\n        # Step 2: Execute the DELETE statement for all ids except the first two\n        cursor.executemany(\"\"\"\n            DELETE FROM message_store\n            WHERE id = ?;\n        \"\"\", [(row[0],) for row in ids_to_delete])\n        \n        conn.commit()\n        print(f\"Deleted {cursor.rowcount} messages.\")\n    else:\n        print(\"No messages to delete.\")\n    \n    # Close the connection\n    conn.close()","metadata":{"execution":{"iopub.status.busy":"2024-10-30T19:37:08.211281Z","iopub.execute_input":"2024-10-30T19:37:08.211670Z","iopub.status.idle":"2024-10-30T19:37:08.224105Z","shell.execute_reply.started":"2024-10-30T19:37:08.211618Z","shell.execute_reply":"2024-10-30T19:37:08.222898Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"### Define Chain","metadata":{}},{"cell_type":"code","source":"def get_llm_chain(key):\n    prefix = CFG.prefixes_map[key]\n    assert prefix != None, CFG.error_messages['prefix'].format(key=key)\n    \n    prompt = ChatPromptTemplate.from_messages(\n        [\n            (\"system\", prefix),\n            MessagesPlaceholder(variable_name=\"history\"),\n            (\"human\", \"{question}\"),\n        ]\n    )\n\n    runnable = prompt | llm\n\n    chain = RunnableWithMessageHistory(\n        runnable,\n        get_session_history,\n        input_messages_key=\"question\",\n        history_messages_key=\"history\",\n    )\n    \n    return chain","metadata":{"execution":{"iopub.status.busy":"2024-10-30T19:37:08.227817Z","iopub.execute_input":"2024-10-30T19:37:08.228365Z","iopub.status.idle":"2024-10-30T19:37:11.843919Z","shell.execute_reply.started":"2024-10-30T19:37:08.228302Z","shell.execute_reply":"2024-10-30T19:37:11.842541Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"# LLM Intergration","metadata":{}},{"cell_type":"markdown","source":"## Get LLM Answer","metadata":{}},{"cell_type":"code","source":"def get_answer(llm, questions, configs):\n    predictions = llm.batch(\n        questions,\n        config=configs,\n    )\n    return [{'text': pred} for pred in predictions]","metadata":{"execution":{"iopub.status.busy":"2024-10-30T19:37:11.845414Z","iopub.execute_input":"2024-10-30T19:37:11.845826Z","iopub.status.idle":"2024-10-30T19:37:11.860542Z","shell.execute_reply.started":"2024-10-30T19:37:11.845785Z","shell.execute_reply":"2024-10-30T19:37:11.859340Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"## Conversations Before Doubt     ","metadata":{}},{"cell_type":"code","source":"# Conversations Before Doubt     \ndef get_conversations_before(key, chain, questions, examples, configs):\n    file_path = f'/kaggle/input/conversations-before/conversations_before_{key}.pkl'\n    if not os.path.exists(file_path):\n        conversations_before = get_answer(chain, examples, configs)\n    else:\n        # Loading conversations_before\n        conversations_before = load_pkl(file_path)\n        if not os.path.exists('/kaggle/working/memory.db'):\n            update_db(questions, conversations_before)\n    return conversations_before","metadata":{"execution":{"iopub.status.busy":"2024-10-30T19:37:11.862032Z","iopub.execute_input":"2024-10-30T19:37:11.862553Z","iopub.status.idle":"2024-10-30T19:37:11.873511Z","shell.execute_reply.started":"2024-10-30T19:37:11.862507Z","shell.execute_reply":"2024-10-30T19:37:11.872314Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"## Extract Only Correct Answers  ","metadata":{}},{"cell_type":"markdown","source":"### Evaluate conversations before  ","metadata":{}},{"cell_type":"code","source":"def evaluate(key, questions, gold_answers):\n    # Initialize QAEvalChain\n    qa_eval_chain = load_eval_llm()\n\n    # Prepare examples (questions with gold answers)\n    if key == 'GSM8K':\n        examples_test = [ {\"question\": q, \"answer\": r.split('#### ')[-1]} for q, r in zip(questions, gold_answers)]\n    else:\n        examples_test = [ {\"question\": q, \"answer\": r} for q, r in zip(questions, gold_answers)]\n\n    # Convert to Datasets objects to improve efficiency\n    examples_test = Dataset.from_list(examples_test)\n    conversations_before_test = Dataset.from_list(conversations_before)\n\n    # Evaluate the model-generated answers by passing 'predictions' separately\n    eval_results = qa_eval_chain.evaluate(examples=examples_test,\n                                          predictions=conversations_before_test,\n                                          question_key=\"question\",\n                                          prediction_key=\"text\")\n\n    return eval_results","metadata":{"execution":{"iopub.status.busy":"2024-10-30T19:37:11.876281Z","iopub.execute_input":"2024-10-30T19:37:11.877438Z","iopub.status.idle":"2024-10-30T19:37:11.888026Z","shell.execute_reply.started":"2024-10-30T19:37:11.877374Z","shell.execute_reply":"2024-10-30T19:37:11.886563Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"### Filter Incorrect Responses","metadata":{}},{"cell_type":"code","source":"def filter_data(key, conversations_before, eval_results, questions, gold_answers, configs):\n    \n    file_path = f'/kaggle/input/filtered-data-before-doubt/{key}'\n    filtered_configs = []\n    if not os.path.exists(file_path):\n        filtered_conversations_before = []\n        filtered_questions = []\n        filtered_gold_answers = []\n        for conv, res, q, a, conf in zip(conversations_before, eval_results, questions, gold_answers, configs):\n            temp = res['results'].lower()\n            if 'correct' in temp and 'incorrect' not in temp:\n                filtered_conversations_before.append(conv)\n                filtered_questions.append(q)\n                filtered_gold_answers.append(a)\n                filtered_configs.append(conf)\n\n    else:\n        # Loading filtered conversations_before\n        filtered_conversations_before = load_pkl(f'/kaggle/input/filtered-data-before-doubt/{key}/filtered_conversations_before_{key}.pkl')\n        # Loading filtered questions\n        filtered_questions = load_pkl(f'/kaggle/input/filtered-data-before-doubt/{key}/filtered_questions_{key}.pkl')\n        # Loading filtered gold answers\n        filtered_gold_answers = load_pkl(f'/kaggle/input/filtered-data-before-doubt/{key}/filtered_gold_answers_{key}.pkl')\n\n        filtered_questions_set = set(filtered_questions)\n        for session_id in range(1, CFG.n+1):\n            q = (get_session_history(f'{session_id}').get_messages()[0]).content\n            if q in filtered_questions_set:\n                filtered_configs.append({\"configurable\": {\"session_id\": f\"{session_id}\"}})\n\n    # build filtered examples\n    filtered_examples = [{\"question\": q} for q in filtered_questions]\n    return filtered_conversations_before, filtered_questions, filtered_gold_answers, filtered_configs, filtered_examples","metadata":{"execution":{"iopub.status.busy":"2024-10-30T19:38:29.466219Z","iopub.execute_input":"2024-10-30T19:38:29.466824Z","iopub.status.idle":"2024-10-30T19:38:29.481649Z","shell.execute_reply.started":"2024-10-30T19:38:29.466762Z","shell.execute_reply":"2024-10-30T19:38:29.480340Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"## Conversations After Doubt","metadata":{}},{"cell_type":"code","source":"def get_conversation_after_doubt(llm, configs, experiment, conversations_before=None):\n    \n    def update_history(_llm, _questions, _configs, history):\n        qs = Dataset.from_list(_questions)\n        preds = get_answer(_llm, qs, _configs)\n        for i, pred in enumerate(preds):\n            history[i].append(pred['text'])\n            \n    if conversations_before is None:\n        history = [[] for _ in range(len(questions))] # idx i: history of question i\n        update_history(llm, questions, configs, history)\n    else:\n        history = [[ans['text']] for ans in conversations_before]\n    \n    for idx, induced_doubt in enumerate(experiment):\n        print(f\"Generateing answers for induced doubt question {idx+1}/{len(experiment)}\")\n        induced_doubts = []\n        for hist in history:\n            hist.append(induced_doubt)\n            induced_doubts.append({\"question\": induced_doubt})\n        update_history(llm, induced_doubts, configs, history)\n    return history","metadata":{"execution":{"iopub.status.busy":"2024-10-30T19:38:29.483114Z","iopub.execute_input":"2024-10-30T19:38:29.483544Z","iopub.status.idle":"2024-10-30T19:38:29.502504Z","shell.execute_reply.started":"2024-10-30T19:38:29.483503Z","shell.execute_reply":"2024-10-30T19:38:29.501228Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":"## Main","metadata":{}},{"cell_type":"code","source":"all_conversations = {}\n\nfor key in CFG.supported_datasets:\n    if key == 'SQuAD_v1':\n        continue\n    print(key)\n    questions = all_questions[key]\n    gold_answers = all_gold_answers[key]\n    examples = all_examples[key]\n    configs = all_configs[key]\n    \n    # Define chain\n    chain = get_llm_chain(key)\n    \n    # Get conversations before and memory database\n    conversations_before = get_conversations_before(key, chain, questions, examples, configs)\n    \n    # Evaluate results and receive filtering initial incorrect responses\n    eval_results = None\n    if not os.path.exists(f'/kaggle/input/filtered-data-before-doubt/{key}'):\n        eval_results = evaluate(key, questions, gold_answers)\n    \n    # Filter incorrect responses \n    data = filter_data(key, conversations_before, eval_results, questions, gold_answers, configs)\n    filtered_conversations_before, filtered_questions, filtered_gold_answers, filtered_configs, filtered_examples = data\n    \n    # Sanity check \n    print('Sanity Check - should print the same lengths:')\n    print(len(filtered_questions), len(filtered_gold_answers), len(filtered_configs))\n    \n    \n    # Get conversations after doubt\n    file_path = f'/kaggle/input/conversations-after/conversations_after_{key}.pkl'\n    if not os.path.exists(file_path):\n        conversations_after = []\n        for idx, exp in enumerate(CFG.experiments):\n            print(f'Experiment {idx+1}/{len(CFG.experiments)}')\n            conversations_after.append(get_conversation_after_doubt(chain, filtered_configs, exp, filtered_conversations_before))\n            # delete experiment history from all sessions expect for the main question and first response\n            delete_except_first_two() \n    else:\n        # Loading the data from a pickle file\n        conversations_after = load_pkl(file_path)\n    \n    # Delete memory database\n    !rm -rf memory.db\n    \n    all_conversations[key] = [filtered_conversations_before, conversations_after, filtered_questions, filtered_gold_answers]","metadata":{"execution":{"iopub.status.busy":"2024-10-30T19:37:11.928089Z","iopub.execute_input":"2024-10-30T19:37:11.928667Z","iopub.status.idle":"2024-10-30T19:38:29.462288Z","shell.execute_reply.started":"2024-10-30T19:37:11.928621Z","shell.execute_reply":"2024-10-30T19:38:29.460601Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"CSQA\nDeleted 6 messages.\nSanity Check - should print the same lengths:\n400 400 400\nGSM8K\nNo messages to delete.\nSanity Check - should print the same lengths:\n220 220 220\nSQuAD_v2\nSanity Check - should print the same lengths:\n191 191 191\nHotpotQA\nSanity Check - should print the same lengths:\n267 267 267\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Save Data For Evaluation","metadata":{}},{"cell_type":"code","source":"# for key in CFG.supported_datasets:\n#     if key == 'SQuAD_v1':\n#         continue\n#     write_to_pkl(f'filtered_conversations_before_{key}', all_conversations[key][0])\n#     write_to_pkl(f'conversations_after_{key}', all_conversations[key][1])\n#     write_to_pkl(f'filtered_questions_{key}', all_conversations[key][2])\n#     write_to_pkl(f'filtered_gold_answers_{key}', all_conversations[key][3])","metadata":{"execution":{"iopub.status.busy":"2024-10-30T19:39:29.826934Z","iopub.execute_input":"2024-10-30T19:39:29.827977Z","iopub.status.idle":"2024-10-30T19:39:29.870923Z","shell.execute_reply.started":"2024-10-30T19:39:29.827926Z","shell.execute_reply":"2024-10-30T19:39:29.869700Z"},"trusted":true},"execution_count":14,"outputs":[]}]}